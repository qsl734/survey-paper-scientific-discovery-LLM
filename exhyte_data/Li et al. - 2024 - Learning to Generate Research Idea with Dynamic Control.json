{
  "Inputs to the workflow": {
    "performed": "Yes",
    "User provide high-level research direction or goal": {
      "performed": "No"
    },
    "User provide structured, domain-specific specifications": {
      "performed": "No"
    },
    "User provide research papers": {
      "performed": "Yes",
      "Format": "Full research papers (ICLR, NeurIPS) including abstracts, methodology, experiments, and reviews.",
      "Example": "Papers from ICLR 2023/2024 and NeurIPS 2023/2024 are collected and used as input.",
      "Role in workflow": "Serve as the primary input for idea generation, fine-tuning, and reward model training."
    },
    "User provide datasets other than research papers": {
      "performed": "No"
    },
    "User provide representations or formal inputs": {
      "performed": "No"
    }
  },
  "Query Structuring": {
    "performed": "Yes",
    "Query Decomposition": {
      "performed": "No"
    },
    "Structural or Entity Decomposition": {
      "performed": "No"
    },
    "Workflow Decomposition": {
      "performed": "No"
    },
    "Textual or Knowledge Embedding": {
      "performed": "Yes",
      "Method details": "LLMs process research papers and ideas, extracting and encoding information for downstream tasks.",
      "Inputs": "Research papers, extracted ideas.",
      "Outputs": "Encoded representations used for fine-tuning and reward modeling.",
      "Example": "LLaMA and Llama-3-70B-Instruct are used to process and score research ideas.",
      "Role in workflow": "Enable semantic understanding and scoring of ideas across novelty, feasibility, and effectiveness."
    },
    "Molecular or Chemical Embedding": {
      "performed": "No"
    },
    "Biological or Phenotypic Embedding": {
      "performed": "No"
    },
    "Pattern and Feature Extraction": {
      "performed": "Yes",
      "Method details": "Extraction of key features (method, experiment plan) from papers using LLM prompts.",
      "Inputs": "Research papers.",
      "Outputs": "Structured method and experiment plan sections.",
      "Example": "Prompted LLaMA extracts method and experiment plan from ICLR papers.",
      "Role in workflow": "Provides structured inputs for supervised fine-tuning and idea generation."
    },
    "Biological Relationship Extraction": {
      "performed": "No"
    },
    "Property and Annotation Extraction": {
      "performed": "No"
    },
    "Sequence and Structure Feature Extraction": {
      "performed": "No"
    }
  },
  "Data Retrieval": {
    "performed": "Yes",
    "Data Retrieval via Multi-Query Generation and Exploration": {
      "performed": "No"
    },
    "Literature and Data Retrieval via APIs": {
      "performed": "Yes",
      "Method details": "APIs such as Semantic Scholar and arXiv are used to collect paper content.",
      "Inputs": "Paper titles, queries.",
      "Outputs": "Full paper texts and metadata.",
      "Example": "Paper content is scraped with title from the Semantic Scholar and arXiv APIs.",
      "Role in workflow": "Provides the raw data for idea extraction, fine-tuning, and evaluation."
    },
    "Data Retrieval with Prioritization and Filtering Agents": {
      "performed": "No"
    },
    "Domain-Specific Data Retrieval and Reasoning": {
      "performed": "No"
    },
    "Code-Driven or Tool-Augmented Data Retrieval": {
      "performed": "No"
    },
    "Literature data Retrieval Citation-Network–Based Expansion": {
      "performed": "Yes",
      "Method details": "Selection of the most significant supporting paper from related works using citation counts.",
      "Inputs": "Related works, citation counts.",
      "Outputs": "Most relevant supporting paper for each target paper.",
      "Example": "The most significant supporting paper is selected based on citation counts within the sampled paper.",
      "Role in workflow": "Provides context for supervised fine-tuning by pairing target papers with key related works."
    },
    "Literature data Retrieval via Semantic and Similarity-Based analysis": {
      "performed": "No"
    },
    "Literature data Retrieval via Multi-Step Reference and Evidence Selection": {
      "performed": "No"
    },
    "Domain-Specific Literature data Retrieval": {
      "performed": "No"
    },
    "Manual and Semi-Automatic Curation of Literature data": {
      "performed": "No"
    },
    "Structural or Similarity-Based Dataset Retrieval": {
      "performed": "No"
    },
    "Data Retrieval via Domain-Specific Repository Querying": {
      "performed": "No"
    },
    "Library Assembly and Data Augmentation": {
      "performed": "No"
    }
  },
  "Knowledge Assembly": {
    "performed": "Yes",
    "Standardized Section Extraction from Literature data": {
      "performed": "Yes",
      "Method details": "Papers are segmented into abstract, methodology, and experiment sections using regular expressions.",
      "Inputs": "Full paper texts.",
      "Outputs": "Structured sections (abstract, methodology, experiments).",
      "Example": "Paper content is cleaned up with regular expressions to extract corresponding sections.",
      "Role in workflow": "Normalizes input for downstream extraction and modeling."
    },
    "Concise Synopsis and Summarization of Literature data": {
      "performed": "Yes",
      "Method details": "LLMs are prompted to extract concise method and experiment plan summaries.",
      "Inputs": "Structured paper sections.",
      "Outputs": "Summarized method and experiment plan.",
      "Example": "LLaMA is prompted to extract and summarize the method and experiment plan.",
      "Role in workflow": "Provides concise, structured inputs for idea generation and evaluation."
    },
    "Facet-Based or Field-Specific Extraction from Literature data": {
      "performed": "Yes",
      "Method details": "Extraction of specific fields (method, experiment plan) via LLM prompts.",
      "Inputs": "Research papers.",
      "Outputs": "Field-specific outputs (method, experiment plan).",
      "Example": "Prompted LLaMA extracts method and experiment plan from each paper.",
      "Role in workflow": "Enables fine-tuning and structured idea generation."
    },
    "Domain-Tailored Extraction from Literature data": {
      "performed": "No"
    },
    "Task/Entity-Centric Knowledge Graphs": {
      "performed": "No"
    },
    "Causal or Relation-Specific Knowledge Graphs": {
      "performed": "No"
    },
    "Biomedical or Domain-Specific Interaction Graphs": {
      "performed": "No"
    },
    "Literature Database Construction": {
      "performed": "Yes",
      "Method details": "Papers from ICLR and NeurIPS are curated, segmented, and stored as structured records.",
      "Inputs": "Collected papers.",
      "Outputs": "Structured literature database for training and evaluation.",
      "Example": "Dataset of 6,765 papers is split for fine-tuning, RL, and evaluation.",
      "Role in workflow": "Supports model training, reward modeling, and evaluation."
    },
    "Entity- or Co-Occurrence–Based Databases": {
      "performed": "No"
    },
    "Reasoning-Chain or Temporal Databases for Literature": {
      "performed": "No"
    }
  },
  "Hypothesis/Idea Generation": {
    "performed": "Yes",
    "Idea/hypothesis generation without additional literature or dataset as context": {
      "performed": "No"
    },
    "LLM Agent Generate ideas/hypotheses via Task Decomposition": {
      "performed": "No"
    },
    "Generate ideas/hypotheses using Domain-Specialized LLM Agent": {
      "performed": "No"
    },
    "Literature data used during idea/hypothesis generation as context": {
      "performed": "Yes",
      "Method details": "Research papers and their extracted sections are used as context for LLM-based idea generation.",
      "Inputs": "Research papers, supporting related works.",
      "Outputs": "Generated research ideas (method, experiment plan, problem, related works).",
      "Example": "LLaMA generates structured research ideas using paper content and related works.",
      "Role in workflow": "Grounds idea generation in existing literature for relevance and novelty."
    },
    "Summarization Literature data used during idea/hypothesis generation": {
      "performed": "Yes",
      "Method details": "Summarized method and experiment plan sections are used as input for idea generation.",
      "Inputs": "Summarized sections from papers.",
      "Outputs": "Structured research ideas.",
      "Example": "LLM uses extracted summaries to generate new research ideas.",
      "Role in workflow": "Condenses relevant information for efficient idea synthesis."
    },
    "Idea/hypothesis generation via Facet Recombination": {
      "performed": "No"
    },
    "Idea/hypothesis generation via contructed Reasoning-Chain from literature": {
      "performed": "No"
    },
    "Idea/hypothesis generation via Knowledge Graph developed from literature": {
      "performed": "No"
    },
    "Idea/hypothesis generation via Pattern Detection from dataset": {
      "performed": "No"
    },
    "Idea/hypothesis generation via Few-Shot Data Seeding": {
      "performed": "No"
    },
    "Idea/hypothesis generation using Observational data": {
      "performed": "No"
    },
    "Idea/hypothesis generation via Feature-Driven Property Prediction": {
      "performed": "No"
    },
    "Idea/hypothesis generation after Fine-Tuning the LLM model": {
      "performed": "Yes",
      "Method details": "LLMs are fine-tuned on paper-idea pairs to improve idea generation quality.",
      "Inputs": "Pairs of research papers and extracted ideas.",
      "Outputs": "Fine-tuned LLM capable of generating higher-quality research ideas.",
      "Example": "LLaMA is fine-tuned on 1,000 ICLR paper-idea pairs.",
      "Role in workflow": "Enhances the relevance and quality of generated ideas."
    }
  },
  "Hypothesis/Idea Prioritization": {
    "performed": "Yes",
    "LLM-based Hypothesis/Idea evaluation via Scientific Quality": {
      "performed": "Yes",
      "Method details": "Reward models (LLMs with MLP heads) are trained to score ideas on novelty, feasibility, and effectiveness.",
      "Inputs": "Generated research ideas, review data, scoring criteria.",
      "Outputs": "Scores for each idea on key metrics.",
      "Example": "Llama-3-70B-Instruct is used to score ideas based on review data and prompts.",
      "Role in workflow": "Guides RL fine-tuning and dynamic control for idea optimization."
    },
    "LLM-based Hypothesis/Idea evaluation via Domain-Specific Evaluation": {
      "performed": "Yes",
      "Method details": "LLMs are prompted to evaluate feasibility and effectiveness based on experiment setup and results.",
      "Inputs": "Research ideas, experiment sections, review comments.",
      "Outputs": "Feasibility and effectiveness scores.",
      "Example": "Feasibility and effectiveness are scored using LLM prompts and review data.",
      "Role in workflow": "Ensures generated ideas are practical and impactful."
    },
    "LLM-based Hypothesis/Idea evaluation via Contextual Evidence Scoring": {
      "performed": "No"
    },
    "LLM-based Hypothesis/Idea evaluation via Interpretability or Success Metrics": {
      "performed": "No"
    },
    "Hypothesis/Idea evaluation via Novelty Checking with Literature Comparison": {
      "performed": "Yes",
      "Method details": "Novelty is scored by comparing generated ideas to existing works using LLMs and review data.",
      "Inputs": "Generated ideas, related works, review comments.",
      "Outputs": "Novelty scores for each idea.",
      "Example": "Novelty scores are assigned based on similarity to prior work and reviewer feedback.",
      "Role in workflow": "Promotes originality in idea generation."
    },
    "Hypothesis/Idea evaluation via Alignment with Literature Chains": {
      "performed": "No"
    },
    "Hypothesis/Idea evaluation via Knowledge-Graph Grounded Similarity Metrics": {
      "performed": "No"
    },
    "Hypothesis/Idea evaluation via Quantitative Assessment Using Domain Metrics": {
      "performed": "No"
    },
    "Hypothesis/Idea evaluation via Human/Expert": {
      "performed": "Yes",
      "Method details": "Domain experts manually evaluate a subset of generated ideas for novelty, feasibility, and effectiveness.",
      "Inputs": "Generated ideas, evaluation criteria.",
      "Outputs": "Human-assigned scores for each metric.",
      "Example": "15 experts assess 30 generated ideas, providing human scores for comparison.",
      "Role in workflow": "Validates and calibrates automated scoring."
    }
  },
  "Test": {
    "performed": "Yes",
    "Experimental Design Generation via literature-Grounded Model/Protocol Selection": {
      "performed": "Yes",
      "Method details": "LLMs extract and generate experiment plans based on methods and protocols from existing papers.",
      "Inputs": "Method and experiment sections from literature.",
      "Outputs": "Experiment plans for new research ideas.",
      "Example": "LLM generates experiment plans using extracted literature protocols.",
      "Role in workflow": "Ensures experimental feasibility and grounding for generated ideas."
    },
    "Experimental Design Generation via Literature Synthesis for New Protocol Generation": {
      "performed": "No"
    },
    "Experimental Design Generation via Few-Shot or Example-Based Prompting": {
      "performed": "No"
    },
    "Experimental Design Generation via Executable Code Generation from Literature": {
      "performed": "No"
    },
    "LLM-Based Experimental Design Generation via Agentic Exploration and Planning": {
      "performed": "No"
    },
    "LLM-Based Experimental Design Generation via Code and Workflow Translation": {
      "performed": "No"
    },
    "LLM-Based Experimental Design Generation via Multi-Agent Planning with Specialized Roles": {
      "performed": "No"
    },
    "LLM-Based Experimental Design Generation via Domain-Specific Experimental Mapping": {
      "performed": "No"
    },
    "Test Execution via Human-in-the-Loop": {
      "performed": "No"
    },
    "Test Execution via Automated Wet-Lab Execution": {
      "performed": "No"
    },
    "Test Execution via Computational or In-Silico": {
      "performed": "No"
    },
    "Refinement via LLM Agent Feedback Loops": {
      "performed": "Yes",
      "Method details": "Dynamic decoding with RNN adjusts controller weights sentence-by-sentence during idea generation.",
      "Inputs": "Generated idea sentences, reward model feedback.",
      "Outputs": "Context-aware, refined research ideas.",
      "Example": "RNN predicts controller weights for each sentence to balance novelty, feasibility, and effectiveness.",
      "Role in workflow": "Enables adaptive, context-sensitive refinement of generated ideas."
    },
    "Refinement via Automated Quality Evaluation using Model-Based Critics": {
      "performed": "No"
    },
    "Refinement via Dynamic Agent Updating Based on Evolving Context or Data": {
      "performed": "No"
    },
    "Refinement via guided by computational-data": {
      "performed": "No"
    },
    "Refinement via experimental validation": {
      "performed": "No"
    },
    "Refinement via Performance-metric": {
      "performed": "No"
    },
    "Refinement via Human–data integration": {
      "performed": "No"
    }
  },
  "paper_title": "Learning to Generate Research Idea with Dynamic Control",
  "authors": [
    "Ruochen",
    "Liqiang",
    "Chi",
    "Jiawei",
    "Xinya"
  ],
  "published": "2024-12-19",
  "link": "http://arxiv.org/abs/2412.14626"
}