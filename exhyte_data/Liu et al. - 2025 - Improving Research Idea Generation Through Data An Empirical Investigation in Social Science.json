{
  "Inputs to the workflow": {
    "performed": "Yes",
    "User provide high-level research direction or goal": {
      "performed": "Yes",
      "Format": "Natural language research topics or prompts",
      "Example": "10 climate negotiation-related research topics generated and manually verified (Appendix Table 8)",
      "Role in workflow": "Defines the research focus for LLM-driven idea generation and evaluation."
    },
    "User provide structured, domain-specific specifications": {
      "performed": "No"
    },
    "User provide research papers": {
      "performed": "Yes",
      "Format": "Full papers, titles, abstracts",
      "Example": "8 manually reviewed climate negotiation papers with clear hypotheses and replicable data (Appendix Table 7)",
      "Role in workflow": "Used for hypothesis validation and as reference for idea evaluation."
    },
    "User provide datasets other than research papers": {
      "performed": "Yes",
      "Format": "CSV datasets, metadata descriptions",
      "Example": "22 datasets in CLIMATEDATABANK, including textual, panel, and cross-sectional data (Appendix Table 6)",
      "Role in workflow": "Provides empirical grounding and enables feasibility checks and validation."
    },
    "User provide representations or formal inputs": {
      "performed": "No"
    }
  },
  "Query Structuring": {
    "performed": "Yes",
    "Query Decomposition": {
      "performed": "Yes",
      "Method details": "LLMs decompose research topics into structured research questions, theories, and hypotheses.",
      "Inputs": "Research topic prompt",
      "Outputs": "Structured idea components (research question, theory, hypotheses)",
      "Example": "Each idea generated contains these three components (see Figure 2, Table 11).",
      "Role in workflow": "Enables systematic idea generation and downstream validation."
    },
    "Structural or Entity Decomposition": {
      "performed": "No"
    },
    "Workflow Decomposition": {
      "performed": "Yes",
      "Method details": "Multi-agent frameworks (e.g., GPT-Researcher) split the process into planner, executor, and publisher roles.",
      "Inputs": "Research topic, literature, metadata",
      "Outputs": "Sequential sub-tasks for idea generation and evaluation",
      "Example": "GPT-Researcher uses planner, executor, publisher agents (Appendix C).",
      "Role in workflow": "Orchestrates multi-step idea generation and evaluation."
    },
    "Textual or Knowledge Embedding": {
      "performed": "No"
    },
    "Molecular or Chemical Embedding": {
      "performed": "No"
    },
    "Biological or Phenotypic Embedding": {
      "performed": "No"
    },
    "Pattern and Feature Extraction": {
      "performed": "No"
    },
    "Biological Relationship Extraction": {
      "performed": "No"
    },
    "Property and Annotation Extraction": {
      "performed": "No"
    },
    "Sequence and Structure Feature Extraction": {
      "performed": "No"
    }
  },
  "Data Retrieval": {
    "performed": "Yes",
    "Data Retrieval via Multi-Query Generation and Exploration": {
      "performed": "Yes",
      "Method details": "LLMs generate multiple research topics and queries for literature and data retrieval.",
      "Inputs": "Research topic prompt",
      "Outputs": "Multiple related queries for literature/data search",
      "Example": "10 research topics generated by GPT-4o for downstream idea generation.",
      "Role in workflow": "Ensures diverse conceptual coverage for idea generation."
    },
    "Literature and Data Retrieval via APIs": {
      "performed": "Yes",
      "Method details": "AI-Researcher retrieves papers from Semantic Scholar via API.",
      "Inputs": "Research topic",
      "Outputs": "Relevant papers (titles, abstracts)",
      "Example": "AI-Researcher retrieves papers for grounding idea generation (Appendix C).",
      "Role in workflow": "Provides literature context for LLM-based idea generation."
    },
    "Data Retrieval with Prioritization and Filtering Agents": {
      "performed": "Yes",
      "Method details": "LLMs rank and select top ideas using tournament ranking and pairwise comparison.",
      "Inputs": "Generated ideas",
      "Outputs": "Top-ranked ideas for evaluation",
      "Example": "Swiss tournament for idea selection (Section 3.3).",
      "Role in workflow": "Filters and prioritizes candidate ideas for downstream evaluation."
    },
    "Domain-Specific Data Retrieval and Reasoning": {
      "performed": "Yes",
      "Method details": "Manual and automated collection of climate negotiation datasets; LLMs select relevant datasets for validation.",
      "Inputs": "Research topic, metadata",
      "Outputs": "Relevant datasets for feasibility and validation",
      "Example": "CLIMATEDATABANK assembled from World Bank, UNFCCC, etc. (Section 2, Table 6).",
      "Role in workflow": "Provides empirical data for feasibility checks and hypothesis validation."
    },
    "Code-Driven or Tool-Augmented Data Retrieval": {
      "performed": "Yes",
      "Method details": "LLMs write and execute Python code to load and analyze datasets for hypothesis validation.",
      "Inputs": "Selected datasets, hypotheses",
      "Outputs": "Validation results, code traces",
      "Example": "GPT-4o with code interpreter assistant runs code for hypothesis validation (Section 4.1, Table 2).",
      "Role in workflow": "Automates empirical validation of hypotheses."
    },
    "Literature data Retrieval Citation-Network–Based Expansion": {
      "performed": "No"
    },
    "Literature data Retrieval via Semantic and Similarity-Based analysis": {
      "performed": "No"
    },
    "Literature data Retrieval via Multi-Step Reference and Evidence Selection": {
      "performed": "No"
    },
    "Domain-Specific Literature data Retrieval": {
      "performed": "Yes",
      "Method details": "Manual curation of domain-specific climate negotiation papers.",
      "Inputs": "Literature review",
      "Outputs": "8 reference papers for validation",
      "Example": "Manual review of 103 papers to select 8 with clear hypotheses (Section 2, Table 7).",
      "Role in workflow": "Provides high-quality ground-truth for validation."
    },
    "Manual and Semi-Automatic Curation of Literature data": {
      "performed": "Yes",
      "Method details": "Manual review and selection of relevant papers for validation experiments.",
      "Inputs": "Corpus of climate negotiation papers",
      "Outputs": "Curated set of 8 papers",
      "Example": "Manual review of 103 papers (Section 2).",
      "Role in workflow": "Ensures quality and relevance of validation set."
    },
    "Structural or Similarity-Based Dataset Retrieval": {
      "performed": "No"
    },
    "Data Retrieval via Domain-Specific Repository Querying": {
      "performed": "Yes",
      "Method details": "Datasets sourced from World Bank Open Data, UNFCCC, ENB, etc.",
      "Inputs": "Domain repositories",
      "Outputs": "CLIMATEDATABANK datasets",
      "Example": "Panel, textual, and cross-sectional data collected (Appendix Table 6).",
      "Role in workflow": "Provides empirical basis for idea feasibility and validation."
    },
    "Library Assembly and Data Augmentation": {
      "performed": "No"
    }
  },
  "Knowledge Assembly": {
    "performed": "Yes",
    "Standardized Section Extraction from Literature data": {
      "performed": "Yes",
      "Method details": "Extraction of titles, abstracts, and hypotheses from papers for use in prompts and validation.",
      "Inputs": "Research papers",
      "Outputs": "Structured fields (titles, abstracts, hypotheses)",
      "Example": "Titles and abstracts used as context for idea generation (Appendix C, Table 7).",
      "Role in workflow": "Normalizes literature for LLM input."
    },
    "Concise Synopsis and Summarization of Literature data": {
      "performed": "Yes",
      "Method details": "LLMs summarize validation traces into concise natural language steps.",
      "Inputs": "Raw code and validation traces",
      "Outputs": "Summarized validation process",
      "Example": "Summarization step in automatic validation (Section 4.2, Table 12).",
      "Role in workflow": "Improves interpretability for downstream selection."
    },
    "Facet-Based or Field-Specific Extraction from Literature data": {
      "performed": "Yes",
      "Method details": "Extraction of research question, theory, and hypotheses as structured idea components.",
      "Inputs": "LLM-generated or literature-based ideas",
      "Outputs": "Structured idea facets",
      "Example": "Each idea contains these fields (Section 3.1, Figure 2).",
      "Role in workflow": "Enables systematic evaluation and validation."
    },
    "Domain-Tailored Extraction from Literature data": {
      "performed": "No"
    },
    "Task/Entity-Centric Knowledge Graphs": {
      "performed": "No"
    },
    "Causal or Relation-Specific Knowledge Graphs": {
      "performed": "No"
    },
    "Biomedical or Domain-Specific Interaction Graphs": {
      "performed": "No"
    },
    "Literature Database Construction": {
      "performed": "No"
    },
    "Entity- or Co-Occurrence–Based Databases": {
      "performed": "No"
    },
    "Reasoning-Chain or Temporal Databases for Literature": {
      "performed": "No"
    }
  },
  "Hypothesis/Idea Generation": {
    "performed": "Yes",
    "Idea/hypothesis generation without additional literature or dataset as context": {
      "performed": "No"
    },
    "LLM Agent Generate ideas/hypotheses via Task Decomposition": {
      "performed": "Yes",
      "Method details": "Multi-agent frameworks (e.g., GPT-Researcher) decompose idea generation into planning, execution, and publishing.",
      "Inputs": "Research topic, literature, metadata",
      "Outputs": "Candidate research ideas",
      "Example": "Planner, executor, publisher agents in GPT-Researcher (Appendix C).",
      "Role in workflow": "Enables modular, multi-step idea generation."
    },
    "Generate ideas/hypotheses using Domain-Specialized LLM Agent": {
      "performed": "No"
    },
    "Literature data used during idea/hypothesis generation as context": {
      "performed": "Yes",
      "Method details": "LLMs use retrieved literature (titles, abstracts) as context for idea generation.",
      "Inputs": "Related literature",
      "Outputs": "Ideas grounded in prior work",
      "Example": "Idea generation prompt includes literature context (Table 13).",
      "Role in workflow": "Ensures ideas are informed by existing research."
    },
    "Summarization Literature data used during idea/hypothesis generation": {
      "performed": "No"
    },
    "Idea/hypothesis generation via Facet Recombination": {
      "performed": "No"
    },
    "Idea/hypothesis generation via contructed Reasoning-Chain from literature": {
      "performed": "No"
    },
    "Idea/hypothesis generation via Knowledge Graph developed from literature": {
      "performed": "No"
    },
    "Idea/hypothesis generation via Pattern Detection from dataset": {
      "performed": "No"
    },
    "Idea/hypothesis generation via Few-Shot Data Seeding": {
      "performed": "No"
    },
    "Idea/hypothesis generation using Observational data": {
      "performed": "No"
    },
    "Idea/hypothesis generation via Feature-Driven Property Prediction": {
      "performed": "No"
    },
    "Idea/hypothesis generation after Fine-Tuning the LLM model": {
      "performed": "No"
    }
  },
  "Hypothesis/Idea Prioritization": {
    "performed": "Yes",
    "LLM-based Hypothesis/Idea evaluation via Scientific Quality": {
      "performed": "Yes",
      "Method details": "LLMs perform pairwise ranking of ideas using criteria: significance, novelty, feasibility, expected effectiveness.",
      "Inputs": "Generated ideas",
      "Outputs": "Ranked list of ideas",
      "Example": "Swiss tournament ranking, ELO scores, Table 10.",
      "Role in workflow": "Selects highest-quality ideas for further evaluation."
    },
    "LLM-based Hypothesis/Idea evaluation via Domain-Specific Evaluation": {
      "performed": "Yes",
      "Method details": "LLMs use domain-specific criteria (e.g., feasibility for social science PhD student) for evaluation.",
      "Inputs": "Ideas, evaluation criteria",
      "Outputs": "Domain-relevant rankings",
      "Example": "Feasibility and expected effectiveness criteria (Section 3.3, Table 9).",
      "Role in workflow": "Ensures ideas are practical and relevant to the field."
    },
    "LLM-based Hypothesis/Idea evaluation via Contextual Evidence Scoring": {
      "performed": "Yes",
      "Method details": "LLMs incorporate summarized validation traces as evidence during idea selection.",
      "Inputs": "Ideas, validation summaries",
      "Outputs": "Evidence-informed rankings",
      "Example": "Idea selection with validation process (Section 4.2, Table 4).",
      "Role in workflow": "Improves selection by grounding in empirical plausibility."
    },
    "LLM-based Hypothesis/Idea evaluation via Interpretability or Success Metrics": {
      "performed": "No"
    },
    "Hypothesis/Idea evaluation via Novelty Checking with Literature Comparison": {
      "performed": "Yes",
      "Method details": "LLMs compare generated ideas to prior literature to assess novelty.",
      "Inputs": "Ideas, literature context",
      "Outputs": "Novelty scores",
      "Example": "Prompt instructs LLMs to avoid incremental modifications and ensure novelty (Table 13).",
      "Role in workflow": "Prevents duplication and encourages original contributions."
    },
    "Hypothesis/Idea evaluation via Alignment with Literature Chains": {
      "performed": "No"
    },
    "Hypothesis/Idea evaluation via Knowledge-Graph Grounded Similarity Metrics": {
      "performed": "No"
    },
    "Hypothesis/Idea evaluation via Quantitative Assessment Using Domain Metrics": {
      "performed": "Yes",
      "Method details": "LLMs use code to quantitatively validate hypotheses against datasets.",
      "Inputs": "Hypotheses, datasets",
      "Outputs": "Supported/not supported labels",
      "Example": "Automatic validation with code interpreter (Section 4.1, Table 2).",
      "Role in workflow": "Provides empirical scoring for hypothesis plausibility."
    },
    "Hypothesis/Idea evaluation via Human/Expert": {
      "performed": "Yes",
      "Method details": "Human annotators evaluate idea pairs and validation processes.",
      "Inputs": "Idea pairs, validation traces",
      "Outputs": "Human rankings and feedback",
      "Example": "Graduate-level annotators assess ideas (Section 3.4, Table 1).",
      "Role in workflow": "Provides external validation and qualitative assessment."
    }
  },
  "Test": {
    "performed": "Yes",
    "Experimental Design Generation via literature-Grounded Model/Protocol Selection": {
      "performed": "No"
    },
    "Experimental Design Generation via Literature Synthesis for New Protocol Generation": {
      "performed": "No"
    },
    "Experimental Design Generation via Few-Shot or Example-Based Prompting": {
      "performed": "No"
    },
    "Experimental Design Generation via Executable Code Generation from Literature": {
      "performed": "No"
    },
    "LLM-Based Experimental Design Generation via Agentic Exploration and Planning": {
      "performed": "No"
    },
    "LLM-Based Experimental Design Generation via Code and Workflow Translation": {
      "performed": "Yes",
      "Method details": "LLMs generate and execute Python code to test hypotheses using provided datasets.",
      "Inputs": "Hypotheses, datasets",
      "Outputs": "Validation traces, supported/not supported results",
      "Example": "GPT-4o with code interpreter assistant runs code for hypothesis validation (Section 4.1, Table 2, Table 12).",
      "Role in workflow": "Automates in-silico testing of hypotheses."
    },
    "LLM-Based Experimental Design Generation via Multi-Agent Planning with Specialized Roles": {
      "performed": "No"
    },
    "LLM-Based Experimental Design Generation via Domain-Specific Experimental Mapping": {
      "performed": "No"
    },
    "Test Execution via Human-in-the-Loop": {
      "performed": "No"
    },
    "Test Execution via Automated Wet-Lab Execution": {
      "performed": "No"
    },
    "Test Execution via Computational or In-Silico": {
      "performed": "Yes",
      "Method details": "LLMs execute code in a sandbox to analyze datasets and validate hypotheses.",
      "Inputs": "Datasets, hypotheses",
      "Outputs": "Empirical validation results",
      "Example": "Automatic validation process using code interpreter (Section 4.1, Table 12).",
      "Role in workflow": "Provides computational evidence for or against hypotheses."
    },
    "Refinement via LLM Agent Feedback Loops": {
      "performed": "No"
    },
    "Refinement via Automated Quality Evaluation using Model-Based Critics": {
      "performed": "No"
    },
    "Refinement via Dynamic Agent Updating Based on Evolving Context or Data": {
      "performed": "No"
    },
    "Refinement via guided by computational-data": {
      "performed": "No"
    },
    "Refinement via experimental validation": {
      "performed": "No"
    },
    "Refinement via Performance-metric": {
      "performed": "No"
    },
    "Refinement via Human–data integration": {
      "performed": "No"
    }
  },
  "paper_title": "Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science",
  "authors": [
    "Xiao",
    "Xinyi",
    "Xinyang",
    "Yansong",
    "Xun"
  ],
  "published": "2025-05-27",
  "link": "http://arxiv.org/abs/2505.21396"
}