{
  "Inputs to the workflow": {
    "performed": "Yes",
    "User provide high-level research direction or goal": {
      "performed": "Yes",
      "Format": "Natural language prompt specifying workshop theme or research topic.",
      "Example": "Workshop theme (e.g., ICBINB focus on negative results) provided as initial prompt.",
      "Role in workflow": "Defines the research direction for idea generation and constrains the scope of autonomous discovery."
    },
    "User provide structured, domain-specific specifications": {
      "performed": "No"
    },
    "User provide research papers": {
      "performed": "No"
    },
    "User provide datasets other than research papers": {
      "performed": "Yes",
      "Format": "Dataset names or links, sometimes manually downloaded.",
      "Example": "Manual download of Crop Pest and Disease Detection dataset from Kaggle for pest detection experiments.",
      "Role in workflow": "Provides empirical data for experimentation and model evaluation."
    },
    "User provide representations or formal inputs": {
      "performed": "No"
    }
  },
  "Query Structuring": {
    "performed": "Yes",
    "Query Decomposition": {
      "performed": "Yes",
      "Method details": "LLM agent generates multiple distinct research ideas from a high-level prompt, each representing a different research direction.",
      "Inputs": "Workshop theme or research topic prompt.",
      "Outputs": "List of candidate research ideas.",
      "Example": "System generates ~20 research ideas per prompt batch.",
      "Role in workflow": "Enables exploration of diverse research avenues before selecting ideas for further development."
    },
    "Structural or Entity Decomposition": {
      "performed": "No"
    },
    "Workflow Decomposition": {
      "performed": "Yes",
      "Method details": "Experiment Progress Manager agent decomposes the experimental process into four stages: preliminary investigation, hyperparameter tuning, research agenda execution, and ablation studies.",
      "Inputs": "Selected research idea.",
      "Outputs": "Structured multi-stage experimental workflow.",
      "Example": "Stages: prototype → hyperparameter tuning → main experiments → ablation.",
      "Role in workflow": "Organizes and manages the progression of experiments for systematic exploration."
    },
    "Textual or Knowledge Embedding": {
      "performed": "No"
    },
    "Molecular or Chemical Embedding": {
      "performed": "No"
    },
    "Biological or Phenotypic Embedding": {
      "performed": "No"
    },
    "Pattern and Feature Extraction": {
      "performed": "No"
    },
    "Biological Relationship Extraction": {
      "performed": "No"
    },
    "Property and Annotation Extraction": {
      "performed": "No"
    },
    "Sequence and Structure Feature Extraction": {
      "performed": "No"
    }
  },
  "Data Retrieval": {
    "performed": "Yes",
    "Data Retrieval via Multi-Query Generation and Exploration": {
      "performed": "Yes",
      "Method details": "LLM agent generates multiple literature search queries during idea generation to assess novelty and context.",
      "Inputs": "Candidate research ideas.",
      "Outputs": "Literature search queries.",
      "Example": "System performs at least one Semantic Scholar search per idea.",
      "Role in workflow": "Ensures proposed ideas are informed by and distinct from existing literature."
    },
    "Literature and Data Retrieval via APIs": {
      "performed": "Yes",
      "Method details": "System uses Semantic Scholar API for literature search during idea generation.",
      "Inputs": "Search queries generated by LLM.",
      "Outputs": "Relevant paper metadata and abstracts.",
      "Example": "Semantic Scholar queried to check novelty of research ideas.",
      "Role in workflow": "Provides literature context for idea selection and refinement."
    },
    "Data Retrieval with Prioritization and Filtering Agents": {
      "performed": "No"
    },
    "Domain-Specific Data Retrieval and Reasoning": {
      "performed": "No"
    },
    "Code-Driven or Tool-Augmented Data Retrieval": {
      "performed": "Yes",
      "Method details": "System generates Python code to download datasets from Hugging Face Hub using datasets.load_dataset.",
      "Inputs": "Dataset names or identifiers.",
      "Outputs": "Downloaded datasets ready for experimentation.",
      "Example": "Automatic loading of SCAN, COGS, or IWSLT datasets.",
      "Role in workflow": "Automates access to standard ML datasets for experiments."
    },
    "Literature data Retrieval Citation-Network–Based Expansion": {
      "performed": "No"
    },
    "Literature data Retrieval via Semantic and Similarity-Based analysis": {
      "performed": "No"
    },
    "Literature data Retrieval via Multi-Step Reference and Evidence Selection": {
      "performed": "No"
    },
    "Domain-Specific Literature data Retrieval": {
      "performed": "No"
    },
    "Manual and Semi-Automatic Curation of Literature data": {
      "performed": "No"
    },
    "Structural or Similarity-Based Dataset Retrieval": {
      "performed": "No"
    },
    "Data Retrieval via Domain-Specific Repository Querying": {
      "performed": "Yes",
      "Method details": "System prompts itself to use Hugging Face Hub for dataset retrieval when possible.",
      "Inputs": "Dataset names.",
      "Outputs": "Datasets loaded via datasets.load_dataset.",
      "Example": "Loading SCAN or COGS datasets for compositionality experiments.",
      "Role in workflow": "Streamlines dataset access for ML research tasks."
    },
    "Library Assembly and Data Augmentation": {
      "performed": "No"
    }
  },
  "Knowledge Assembly": {
    "performed": "No"
  },
  "Hypothesis/Idea Generation": {
    "performed": "Yes",
    "Idea/hypothesis generation without additional literature or dataset as context": {
      "performed": "Yes",
      "Method details": "LLM generates initial research ideas based on internal knowledge and user prompt before literature search.",
      "Inputs": "Workshop theme or research topic.",
      "Outputs": "List of research ideas.",
      "Example": "First batch of ~20 ideas generated per prompt.",
      "Role in workflow": "Seeds the discovery process with candidate hypotheses."
    },
    "LLM Agent Generate ideas/hypotheses via Task Decomposition": {
      "performed": "No"
    },
    "Generate ideas/hypotheses using Domain-Specialized LLM Agent": {
      "performed": "No"
    },
    "Literature data used during idea/hypothesis generation as context": {
      "performed": "Yes",
      "Method details": "System performs literature search via Semantic Scholar and incorporates results to refine and filter ideas.",
      "Inputs": "Candidate ideas and retrieved literature.",
      "Outputs": "Refined research ideas with novelty assessment.",
      "Example": "System checks for novelty and overlap with prior work before finalizing ideas.",
      "Role in workflow": "Ensures ideas are informed by and distinct from existing research."
    },
    "Summarization Literature data used during idea/hypothesis generation": {
      "performed": "No"
    },
    "Idea/hypothesis generation via Facet Recombination": {
      "performed": "No"
    },
    "Idea/hypothesis generation via contructed Reasoning-Chain from literature": {
      "performed": "No"
    },
    "Idea/hypothesis generation via Knowledge Graph developed from literature": {
      "performed": "No"
    },
    "Idea/hypothesis generation via Pattern Detection from dataset": {
      "performed": "No"
    },
    "Idea/hypothesis generation via Few-Shot Data Seeding": {
      "performed": "No"
    },
    "Idea/hypothesis generation using Observational data": {
      "performed": "No"
    },
    "Idea/hypothesis generation via Feature-Driven Property Prediction": {
      "performed": "No"
    },
    "Idea/hypothesis generation after Fine-Tuning the LLM model": {
      "performed": "No"
    }
  },
  "Hypothesis/Idea Prioritization": {
    "performed": "Yes",
    "LLM-based Hypothesis/Idea evaluation via Scientific Quality": {
      "performed": "Yes",
      "Method details": "LLM agent reflects on and critiques generated ideas for novelty, feasibility, and clarity before finalization.",
      "Inputs": "Generated research ideas.",
      "Outputs": "Refined and prioritized ideas.",
      "Example": "Reflection prompt asks LLM to assess novelty and feasibility after each idea.",
      "Role in workflow": "Filters and improves candidate ideas before experimentation."
    },
    "LLM-based Hypothesis/Idea evaluation via Domain-Specific Evaluation": {
      "performed": "No"
    },
    "LLM-based Hypothesis/Idea evaluation via Contextual Evidence Scoring": {
      "performed": "No"
    },
    "LLM-based Hypothesis/Idea evaluation via Interpretability or Success Metrics": {
      "performed": "No"
    },
    "Hypothesis/Idea evaluation via Novelty Checking with Literature Comparison": {
      "performed": "Yes",
      "Method details": "System uses literature search results to check for overlap and novelty relative to prior work.",
      "Inputs": "Candidate ideas and retrieved literature.",
      "Outputs": "Novelty assessment for each idea.",
      "Example": "Semantic Scholar search performed before finalizing idea.",
      "Role in workflow": "Prevents redundant or already-solved research directions."
    },
    "Hypothesis/Idea evaluation via Alignment with Literature Chains": {
      "performed": "No"
    },
    "Hypothesis/Idea evaluation via Knowledge-Graph Grounded Similarity Metrics": {
      "performed": "No"
    },
    "Hypothesis/Idea evaluation via Quantitative Assessment Using Domain Metrics": {
      "performed": "No"
    },
    "Hypothesis/Idea evaluation via Human/Expert": {
      "performed": "No"
    }
  },
  "Test": {
    "performed": "Yes",
    "Experimental Design Generation via literature-Grounded Model/Protocol Selection": {
      "performed": "No"
    },
    "Experimental Design Generation via Literature Synthesis for New Protocol Generation": {
      "performed": "No"
    },
    "Experimental Design Generation via Few-Shot or Example-Based Prompting": {
      "performed": "No"
    },
    "Experimental Design Generation via Executable Code Generation from Literature": {
      "performed": "No"
    },
    "LLM-Based Experimental Design Generation via Agentic Exploration and Planning": {
      "performed": "Yes",
      "Method details": "Experiment Progress Manager agent coordinates multi-stage experimental planning and execution using agentic tree search.",
      "Inputs": "Selected research idea.",
      "Outputs": "Concrete experimental plans and code for each stage.",
      "Example": "Stages: prototype, hyperparameter tuning, main experiments, ablation studies.",
      "Role in workflow": "Enables systematic, autonomous exploration of experimental space."
    },
    "LLM-Based Experimental Design Generation via Code and Workflow Translation": {
      "performed": "Yes",
      "Method details": "LLM generates Python code for experiments, data analysis, and visualization directly from experimental plans.",
      "Inputs": "Experimental plans from agentic tree search.",
      "Outputs": "Executable Python scripts.",
      "Example": "Code for model training, evaluation, and plotting generated and executed automatically.",
      "Role in workflow": "Automates the implementation and execution of experiments."
    },
    "LLM-Based Experimental Design Generation via Multi-Agent Planning with Specialized Roles": {
      "performed": "No"
    },
    "LLM-Based Experimental Design Generation via Domain-Specific Experimental Mapping": {
      "performed": "No"
    },
    "Test Execution via Human-in-the-Loop": {
      "performed": "No"
    },
    "Test Execution via Automated Wet-Lab Execution": {
      "performed": "No"
    },
    "Test Execution via Computational or In-Silico": {
      "performed": "Yes",
      "Method details": "All experiments are executed computationally using generated Python code in a controlled environment.",
      "Inputs": "Generated code and datasets.",
      "Outputs": "Experimental results, metrics, and plots.",
      "Example": "Model training, evaluation, and ablation studies run autonomously.",
      "Role in workflow": "Validates hypotheses and collects data for analysis and manuscript writing."
    },
    "Refinement via LLM Agent Feedback Loops": {
      "performed": "Yes",
      "Method details": "LLM and Vision-Language Model (VLM) agents iteratively review and critique figures, captions, and manuscript drafts.",
      "Inputs": "Generated figures, captions, and text.",
      "Outputs": "Revised figures and improved manuscript drafts.",
      "Example": "VLM flags unclear plots; LLM revises code or captions accordingly.",
      "Role in workflow": "Improves clarity, correctness, and presentation of results."
    },
    "Refinement via Automated Quality Evaluation using Model-Based Critics": {
      "performed": "Yes",
      "Method details": "VLMs evaluate visualizations for clarity, alignment, and duplication; LLMs select best-performing experimental nodes.",
      "Inputs": "Generated plots and experimental outputs.",
      "Outputs": "Feedback for further refinement or debugging.",
      "Example": "VLM detects missing legends or unclear labels in plots.",
      "Role in workflow": "Ensures high-quality, publication-ready outputs."
    },
    "Refinement via Dynamic Agent Updating Based on Evolving Context or Data": {
      "performed": "No"
    },
    "Refinement via guided by computational-data": {
      "performed": "Yes",
      "Method details": "Experimental results (e.g., performance metrics, training curves) guide selection and refinement of subsequent experiments.",
      "Inputs": "Performance metrics and experimental outputs.",
      "Outputs": "Selection of best nodes and further experiment refinement.",
      "Example": "Best-performing node from each stage seeds the next stage.",
      "Role in workflow": "Drives iterative improvement based on empirical results."
    },
    "Refinement via experimental validation": {
      "performed": "No"
    },
    "Refinement via Performance-metric": {
      "performed": "Yes",
      "Method details": "LLM selects best-performing experimental nodes based on metrics such as accuracy, loss, or plot quality.",
      "Inputs": "Performance metrics from experiments.",
      "Outputs": "Selection of nodes for further refinement.",
      "Example": "Best node chosen to proceed to next experimental stage.",
      "Role in workflow": "Focuses resources on most promising experimental directions."
    },
    "Refinement via Human–data integration": {
      "performed": "No"
    }
  },
  "paper_title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
  "authors": [
    "Yutaro",
    "Robert Tjarko",
    "Cong",
    "Shengran",
    "Chris",
    "Jakob",
    "Jeff",
    "David"
  ],
  "published": "2025-04-10",
  "link": "http://arxiv.org/abs/2504.08066"
}