{
  "objective": {
    "answer": "The primary objective of the paper is to address challenges in training robust language models for language-molecule translation by deploying a novel training approach called contrastive preference optimisation, which avoids generating translations that are merely adequate but not perfect.",
    "evidence": "In this context, we focus on machine language-molecule translation and deploy a novel training approach called contrastive preference optimisation, which avoids generating translations that are merely adequate but not perfect."
  },
  "knowledge_gap": {
    "answer": "Existing approaches in language-molecule translation rely on larger models and datasets, which do not necessarily guarantee higher performance and often require exponentially more data than typically used in NLP tasks.",
    "evidence": "In light of the recent significant advancements in the field, none of the above approaches effectively tackle the inherent challenges in training such models. Instead, they rely on sparse or noisy synthetic data, often necessitating exponentially more data than is typically used in NLP tasks."
  },
  "novelty": {
    "answer": [
      "Introduction of contrastive preference optimisation (CTO) for training language-molecule translation models.",
      "Development of a fine-grained, domain-agnostic evaluation method to assess hallucination in LLMs."
    ],
    "evidence": [
      "We deploy a novel way of training LLMs for language-molecule translation that avoids generating translations that are only adequate but not perfect, called contrastive preference optimisation (CTO).",
      "Finally, we propose a fine-grained, domain-agnostic evaluation method to assess hallucination in LLMs and promote responsible use."
    ]
  },
  "inspirational_papers": {
    "answer": "- Edwards et al. (2024) The L+M-24 dataset was used for training and evaluation. (Experimental baselines)\n- Xu et al. (2024) Their work on contrastive preference optimisation inspired our training approach. (Methodological precursors)",
    "evidence": "To ensure that our models can effectively generalise instead of memorising patterns, we conduct experiments using only 10% of the L+M-24 dataset (Edwards et al., 2024). CTO is based on offline preferences instead of supervised fine-tuning, mimicking reference translations (Xu et al., 2024)."
  },
  "method": {
    "steps": [
      {
        "step": "Formulate the language-molecule translation task as a cross-modal translation task using offline preference data.",
        "input": "Pairs of source and target sequences mapped to X and Y spaces, offline preference data D = {x(i), y(i)w , y(i)l }N i=1.",
        "output": "An optimal function f : X ↔Y through a model πθ parameterised by θ.",
        "evidence": "We cast the problem of language-molecule translation as a cross-modal translation task that operates on offline preference data D = {x(i), y(i)w , y(i)l }N i=1."
      },
      {
        "step": "Apply contrastive preference optimisation (CTO) to train the model.",
        "input": "Offline preference data, uniform reference model, parameterised model πθ, and hyperparameter β.",
        "output": "A model trained to generate translations matching a uniform distribution of possible translations.",
        "evidence": "Contrastive preference optimisation (CTO) addresses challenges stemming from the inherent limitation in RLHF, as discussed in § 2, and from the necessity of high-quality data."
      },
      {
        "step": "Introduce a behaviour cloning (BC) regulariser to maintain πθ close to the preferred data distribution.",
        "input": "Preferred data distribution, small positive constant ϵ.",
        "output": "A regularised model with enhanced CPO loss.",
        "evidence": "To maintain πθ close to the preferred data distribution, a behaviour cloning (BC) regulariser is introduced."
      },
      {
        "step": "Evaluate the model using a fine-grained evaluation methodology for assessing hallucinations.",
        "input": "Generated outputs, reference texts, QAFactEval metric, Chr-F metric.",
        "output": "Evaluation results on factual consistency and character n-gram matches.",
        "evidence": "We address these limitations by introducing a scalable fine-grained evaluation methodology for assessing the presence of hallucinations in generated outputs."
      }
    ],
    "tools": [
      {
        "name": "QAFactEval",
        "description": "Used to evaluate the factual consistency of generated captions.",
        "evidence": "For molecule-to-language translation, we deploy the QAFactEval (Fabbri et al., 2022) metric to evaluate the factual consistency of generated captions."
      },
      {
        "name": "Chr-F metric",
        "description": "Used to evaluate character n-gram matches between prediction-reference pairs.",
        "evidence": "For language-to-molecule translation, we employ the Chr-F metric, an F-score statistic, to evaluate character n-gram matches between prediction-reference pairs."
      }
    ],
    "benchmark_datasets": [
      {
        "name": "L+M-24",
        "data_description": "A dataset encompassing both molecule and linguistic modalities.",
        "usage": "Used for training and evaluation of the models.",
        "evidence": "We conduct experiments on the L+M-24 benchmark dataset, which encompasses both molecule and linguistic modalities (Edwards et al., 2024)."
      }
    ],
    "evaluation_metrics": [
      {
        "name": "QAFactEval",
        "purpose": "Measures the factual consistency of generated captions.",
        "application": "Used to evaluate molecule-to-language translation.",
        "evidence": "For molecule-to-language translation, we deploy the QAFactEval (Fabbri et al., 2022) metric to evaluate the factual consistency of generated captions."
      },
      {
        "name": "Chr-F",
        "purpose": "Measures character n-gram matches between prediction-reference pairs.",
        "application": "Used to evaluate language-to-molecule translation.",
        "evidence": "For language-to-molecule translation, we employ the Chr-F metric, an F-score statistic, to evaluate character n-gram matches between prediction-reference pairs."
      }
    ]
  },
  "method_type": {
    "methods": [
      {
        "name": "Experimental design generation",
        "description": "The approach includes producing experimental protocols, configurations, or evaluation strategies.",
        "evidence": "We conduct experiments using only 10% of the data."
      },
      {
        "name": "Iterative refinement of Ideas, Hypothesis and Experiment design",
        "description": "The approach involves refining the experimental setup and evaluation methods.",
        "evidence": "We propose a fine-grained evaluation method that is domain-independent, assessing factual consistency in generated captions using a question-answering evaluation metric."
      }
    ]
  },
  "subject_area": {
    "areas": [
      {
        "name": "Chemical Sciences",
        "description": "The paper focuses on language-molecule translation, which is directly related to chemistry.",
        "evidence": "The field of chemistry and Artificial Intelligence (AI) intersection is an area of active research that aims to accelerate scientific discovery."
      },
      {
        "name": "Applied Sciences & Engineering",
        "description": "The integration of large language models with scientific modalities is an engineering challenge.",
        "evidence": "The integration of large language models (LLMs) with scientific modalities has shown significant promise in this endeavour."
      }
    ]
  },
  "performance_summary": {
    "performance_summary": [
      {
        "summary": "The proposed models achieved up to a 32% improvement compared to counterpart models.",
        "evidence": "Our results demonstrate that our models achieve up to a 32% improvement compared to counterpart models."
      }
    ],
    "baselines": [
      {
        "name": "Meditron",
        "description": "A baseline model trained on the entire dataset for language-molecule translation.",
        "evidence": "Our empirical results demonstrate that our models consistently outperform the leading baseline, Meditron, which is trained on the entire dataset."
      },
      {
        "name": "TxtChem-T5",
        "description": "A T5 model trained on both linguistic and molecule modalities.",
        "evidence": "TxtChem-T5: A T5 model trained on both linguistic and molecule modalities with a multi-task objective across various datasets."
      }
    ],
    "benchmark_datasets": [
      {
        "name": "L+M-24",
        "data_description": "A dataset encompassing both molecule and linguistic modalities.",
        "usage": "Used for training and evaluation of the models.",
        "evidence": "We conduct experiments on the L+M-24 benchmark dataset, which encompasses both molecule and linguistic modalities (Edwards et al., 2024)."
      }
    ],
    "evaluation_metrics": [
      {
        "name": "QAFactEval",
        "purpose": "Measures the factual consistency of generated captions.",
        "application": "Used to evaluate molecule-to-language translation.",
        "evidence": "For molecule-to-language translation, we deploy the QAFactEval (Fabbri et al., 2022) metric to evaluate the factual consistency of generated captions."
      },
      {
        "name": "Chr-F",
        "purpose": "Measures character n-gram matches between prediction-reference pairs.",
        "application": "Used to evaluate language-to-molecule translation.",
        "evidence": "For language-to-molecule translation, we employ the Chr-F metric, an F-score statistic, to evaluate character n-gram matches between prediction-reference pairs."
      }
    ]
  },
  "benchmark_dataset": {
    "name": "L+M-24",
    "data_description": "A dataset encompassing both molecule and linguistic modalities.",
    "usage": "Used for training and evaluation of the models.",
    "evidence": "We conduct experiments on the L+M-24 benchmark dataset, which encompasses both molecule and linguistic modalities (Edwards et al., 2024)."
  },
  "limitations": {
    "limitations": [
      {
        "name": "Limited Data Usage",
        "description": "The study uses only 10% of the available data, which may limit the generalizability of the findings.",
        "evidence": "To ensure that our models can effectively generalise instead of memorising patterns, we conduct experiments using only 10% of the L+M-24 dataset."
      },
      {
        "name": "Challenge in Learning Molecular Patterns",
        "description": "The model struggled to learn molecular patterns when initialized from agnostic cross-modals.",
        "evidence": "However, even though our model achieved better performance compared to Meditron when initialised from agnostic cross-modals, it struggled to learn molecular patterns."
      }
    ]
  },
  "future_directions": {
    "future_directions": [
      {
        "name": "Explore Advanced Initialisation Methods",
        "description": "The authors plan to explore more advanced initialisation methods to improve learning of molecular patterns.",
        "evidence": "In the future, we aim to explore more advanced initialised methods to address this challenge."
      }
    ]
  },
  "resource_link": {
    "answer": "",
    "evidence": ""
  }
}