{
  "objective": {
    "answer": "The primary objective of the paper is to employ Large Language Models (LLMs) to identify neglected climate innovations within scientific literature, particularly focusing on UK-based solutions, to enhance climate action strategies.",
    "evidence": "To address this gap, this study employs a curated dataset sourced from OpenAlex, a comprehensive repository of scientific papers. Utilising Large Language Models (LLMs), such as GPT4-o from OpenAI, we evaluate title-abstract pairs from scientific papers on seven dimensions, covering climate change mitigation potential, stage of technological development, and readiness for deployment."
  },
  "knowledge_gap": {
    "answer": "There is a gap in using LLMs to systematically identify climate innovations within a specific region, which this study addresses by focusing on the UK.",
    "evidence": "However, there is a gap in using LLMs to systematically identify climate innovations within a specific region. This study addresses this gap by using LLM evaluators to identify potential climate innovations in the UK, creating a workflow that can be applied to other regions and serve as the foundation for future work."
  },
  "novelty": {
    "answer": [
      "The use of LLMs to systematically identify climate innovations in scientific literature, focusing on a specific region like the UK.",
      "The development of a workflow that can be applied to other regions for identifying climate innovations.",
      "The comparison of LLM outputs with human evaluations to assess the effectiveness of LLMs in identifying overlooked innovations."
    ],
    "evidence": [
      "This study addresses this gap by using LLM evaluators to identify potential climate innovations in the UK, creating a workflow that can be applied to other regions and serve as the foundation for future work.",
      "Benchmarking the outputs of the LLMs against parallel human evaluations, we aim to assess the effectiveness of these models in finding overlooked innovations and identify any potential advantages over human reasoning."
    ]
  },
  "inspirational_papers": {
    "answer": "- Priem et al. (2022) OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts. (Methodological precursors)",
    "evidence": "We leverage OpenAlex [8], a comprehensive open dataset of scholarly papers and comprehensive meta-data, to provide test data for analysis by state-of-the-art language models, such as GPT4-o from OpenAI."
  },
  "method": {
    "steps": [
      {
        "step": "Data collection from OpenAlex",
        "input": "OpenAlex database",
        "output": "A dataset of approximately 133,619 works",
        "evidence": "To collect relevant research papers, we utilised the OpenAlex database."
      },
      {
        "step": "Filtering and selection of abstracts",
        "input": "Dataset of 133,619 works",
        "output": "A final input dataset of 101,374 works",
        "evidence": "From this curated dataset, we removed works lacking a listed abstract text or topic classification metadata."
      },
      {
        "step": "Human evaluation via survey",
        "input": "100 selected abstracts",
        "output": "Binary responses to seven key questions",
        "evidence": "We implemented a survey using Qualtrics to collect human evaluations for the 100 selected abstracts."
      },
      {
        "step": "LLM evaluation",
        "input": "100 selected abstracts",
        "output": "Binary responses to seven key questions",
        "evidence": "In parallel with the human evaluation, we used a large language model (LLM) to analyse the same 100 abstracts and answer the same seven questions."
      },
      {
        "step": "Data processing and analysis",
        "input": "Responses from human and LLM evaluations",
        "output": "Comparison of LLM's responses against human evaluations",
        "evidence": "For both human and LLM evaluations, the responses were recorded in a structured dataset."
      }
    ],
    "tools": [
      {
        "name": "OpenAlex",
        "description": "Used for collecting relevant research papers",
        "evidence": "To collect relevant research papers, we utilised the OpenAlex database."
      },
      {
        "name": "GPT4-o",
        "description": "Used for evaluating title-abstract pairs from scientific papers",
        "evidence": "Utilising Large Language Models (LLMs), such as GPT4-o from OpenAI, we evaluate title-abstract pairs from scientific papers."
      }
    ],
    "benchmark_datasets": [],
    "evaluation_metrics": [
      {
        "name": "Cohen’s Kappa",
        "purpose": "Evaluates the level of agreement between two raters beyond what would be expected by chance",
        "application": "Computed for each of the seven questions to compare human responses with those generated by the three LLM scenarios",
        "evidence": "Cohen’s Kappa (κ), a statistical measure that evaluates the level of agreement between two raters beyond what would be expected by chance, was computed for each of the seven questions to compare human responses with those generated by the three LLM scenarios."
      }
    ]
  },
  "method_type": {
    "methods": [
      {
        "name": "Literature or Dataset Retrieval",
        "description": "The approach includes retrieving relevant datasets or literature for analysis.",
        "evidence": "To collect relevant research papers, we utilised the OpenAlex database."
      },
      {
        "name": "Knowledge Extraction and Structurization",
        "description": "The approach involves extracting and structuring knowledge from the literature.",
        "evidence": "Utilising Large Language Models (LLMs), such as GPT4-o from OpenAI, we evaluate title-abstract pairs from scientific papers on seven dimensions."
      }
    ]
  },
  "subject_area": {
    "areas": [
      {
        "name": "Earth & Environmental Sciences",
        "description": "The paper focuses on identifying climate innovations to enhance climate action strategies.",
        "evidence": "This work contributes to the discovery of neglected innovations in scientific literature and demonstrates the potential of AI in enhancing climate action strategies."
      },
      {
        "name": "Interdisciplinary Sciences",
        "description": "The methodology applies AI and machine learning to extract knowledge from scientific literature across various disciplines.",
        "evidence": "Unlike the clear relationship between life sciences and biotech innovation, for example, there is no one academic field that dominates climate innovation."
      }
    ]
  },
  "performance_summary": {
    "performance_summary": [
      {
        "summary": "The LLM scenarios successfully identified all 5 positive control abstracts, showing potential advantages over human reasoning.",
        "evidence": "The 5 positive control abstracts with known linkages to climate tech spin-outs were successfully identified by all three LLM scenarios (no shot, context, few-shot), arguably more convincingly than the 6 human survey participants."
      }
    ],
    "baselines": [
      {
        "name": "Human Evaluation",
        "description": "Used as a baseline for comparison with LLM evaluations.",
        "evidence": "Benchmarking the outputs of the LLMs against parallel human evaluations, we aim to assess the effectiveness of these models in finding overlooked innovations."
      }
    ],
    "benchmark_datasets": [],
    "evaluation_metrics": [
      {
        "name": "Cohen’s Kappa",
        "purpose": "Measures the level of agreement between two raters beyond what would be expected by chance.",
        "application": "Used to compare human responses with those generated by the three LLM scenarios.",
        "evidence": "Cohen’s Kappa (κ), a statistical measure that evaluates the level of agreement between two raters beyond what would be expected by chance, was computed for each of the seven questions to compare human responses with those generated by the three LLM scenarios."
      }
    ]
  },
  "benchmark_dataset": null,
  "limitations": {
    "limitations": [
      {
        "name": "Limited Resolution of LLM Scoring",
        "description": "The deterministic reasoning and binary scoring of the LLM present challenges for abstract identification, restricting the resolution of the algorithm.",
        "evidence": "12 of the 25 abstracts passed by the LLM according to Q1 threshold shared the maximum weighted score, rendering them indistinguishable by means of ranking."
      }
    ]
  },
  "future_directions": {
    "future_directions": [
      {
        "name": "Experiment with Different LLMs",
        "description": "Future work will involve experimenting with different LLMs beyond GPT-4o to enhance accuracy and diversity of responses.",
        "evidence": "Future work will involve experimenting with different LLMs beyond GPT-4o, such as LLaMA or other domain-specific models, to enhance the accuracy and diversity of responses."
      },
      {
        "name": "Incorporate Retrieval-Augmented Generation",
        "description": "Incorporate Retrieval-Augmented Generation to provide LLMs with more extensive context, potentially increasing the quality of answers.",
        "evidence": "We will also incorporate Retrieval-Augmented Generation (RAG) to provide the LLMs with a more extensive context, potentially increasing the quality of answers."
      },
      {
        "name": "Develop a Relational Layer",
        "description": "Develop a relational layer to link papers with their authors, grants, and social media presence to identify commercially viable papers and entrepreneurial potential.",
        "evidence": "Furthermore, we are developing a relational layer to link papers with their authors, grants, and social media presence."
      }
    ]
  },
  "resource_link": {
    "answer": "https://github.com/J535D165/pyalex",
    "evidence": "The data was downloaded using the pyalex Python Library and https://github.com/J535D165/pyalex, using the query in 1."
  }
}