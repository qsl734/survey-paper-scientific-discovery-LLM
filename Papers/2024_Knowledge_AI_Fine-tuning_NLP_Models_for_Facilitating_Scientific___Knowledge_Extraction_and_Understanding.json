{
  "objective": {
    "answer": "The primary objective of the paper is to investigate the efficacy of Large Language Models (LLMs) in understanding and extracting scientific knowledge across specific domains and to create a deep learning framework called Knowledge AI. The authors aim to fine-tune pre-trained models on scientific datasets for tasks such as summarization, text generation, question answering, and named entity recognition to improve their applicability in scientific contexts.",
    "evidence": "This project investigates the efficacy of Large Language Models (LLMs) in understanding and extracting scientific knowledge across specific domains and to create a deep learning framework: Knowledge AI."
  },
  "knowledge_gap": {
    "answer": "The paper addresses the gap of a lack of accessible tools that effectively bridge the communication divide between researchers and the broader public, which hinders the dissemination of scientific ideas and discoveries.",
    "evidence": "However, within the realm of scientific research, a critical gap exists – a lack of accessible tools that effectively bridge the communication divide between researchers and the broader public."
  },
  "novelty": {
    "answer": [
      "Development of a unified AI framework specifically designed for scientific tasks using fine-tuned LLMs.",
      "Application of an Adaptive Tokenization Strategy for summarization tasks to manage long documents.",
      "Use of Low Rank adaptations in training causal language models for text generation.",
      "Implementation of extractive k-shot learning with fine-tuned and generative models for question answering."
    ],
    "evidence": [
      "To address this challenge, there is an urgent need for a unified AI framework specifically designed for scientific tasks.",
      "In the summarization task, advanced models are fine-tuned using an Adaptive Tokenization Strategy to address challenges in scientific text summarization.",
      "Text generation explores additional techniques such as Low Rank adaptations in training causal language models.",
      "Question answering explores Extractive and Abstractive QA by testing extractive k-shot learning with fine-tuned and generative models."
    ]
  },
  "inspirational_papers": {
    "answer": "- Taylor et al. (2022) Galactica: A large language model for science. (Methodological precursors)",
    "evidence": "We discovered only recently that Meta AI also released an LLM, Galactica in 2022 specifically designed for scientific tasks. The motivations for the models were similar in spirit to our goals of knowledge extraction and dissemination."
  },
  "method": {
    "steps": [
      {
        "step": "Fine-tuning models for summarization using Adaptive Tokenization Strategy.",
        "input": "Scientific papers dataset from Hugging Face.",
        "output": "Enhanced accuracy and coherence of summaries.",
        "evidence": "In the summarization task, advanced models are fine-tuned using an Adaptive Tokenization Strategy to address challenges in scientific text summarization."
      },
      {
        "step": "Fine-tuning models for text generation using Low Rank adaptations.",
        "input": "ArXIV dataset sampled into 256-token segments.",
        "output": "Improved text generation capabilities.",
        "evidence": "Text generation explores additional techniques such as Low Rank adaptations in training causal language models."
      },
      {
        "step": "Fine-tuning models for question answering using extractive k-shot learning.",
        "input": "SQUAD and PubMedQA datasets.",
        "output": "Improved model's latent ability to answer short-form questions.",
        "evidence": "Question answering explores Extractive and Abstractive QA by testing extractive k-shot learning with fine-tuned and generative models."
      },
      {
        "step": "Fine-tuning models for NER on scientific datasets.",
        "input": "CoNLL2003, SciERC, and GENIA datasets.",
        "output": "Improved token classification of named entities.",
        "evidence": "NER fine-tunes BERT, SciBERT, and SciDeBERTa against a set of increasing science-focused NER datasets, CoNLL2003, SciERC and GENIA."
      }
    ],
    "tools": [
      {
        "name": "BART and LED models",
        "description": "Used for summarization tasks.",
        "evidence": "Summarization examines fine-tuned BART and LED models."
      },
      {
        "name": "distilgpt2",
        "description": "Used for text generation tasks.",
        "evidence": "Text generation uses distilgpt2."
      },
      {
        "name": "BERT and SciBERT",
        "description": "Used for question answering tasks.",
        "evidence": "Question answering fine-tunes two sets of models, BERT and SciBERT."
      },
      {
        "name": "SciDeBERTa",
        "description": "Used for NER tasks.",
        "evidence": "NER fine-tunes BERT, SciBERT, and SciDeBERTa."
      }
    ],
    "benchmark_datasets": [
      {
        "name": "SQUAD",
        "data_description": "Dataset for extractive question answering.",
        "usage": "Used for fine-tuning models for extractive QA.",
        "evidence": "Question answering fine-tunes two sets of models, BERT and SciBERT are fine-tuned on SQUAD for Extractive."
      },
      {
        "name": "PubMedQA",
        "data_description": "Dataset for abstractive question answering.",
        "usage": "Used for fine-tuning models for abstractive QA.",
        "evidence": "Question answering fine-tunes two sets of models, BERT and SciBERT are fine-tuned on PubMedQA for Abstractive."
      },
      {
        "name": "CoNLL2003",
        "data_description": "Dataset for named entity recognition.",
        "usage": "Used for fine-tuning models for NER.",
        "evidence": "NER fine-tunes BERT, SciBERT, and SciDeBERTa against a set of increasing science-focused NER datasets, CoNLL2003."
      }
    ],
    "evaluation_metrics": [
      {
        "name": "ROUGE",
        "purpose": "Measures overlap between generated and reference texts.",
        "application": "Used to assess text generation and summarization tasks.",
        "evidence": "We employ different metrics to assess the finetuning and model performance on the respective tasks. For text generation, summarization, and Q & A, we measure the various ROUGE scores."
      },
      {
        "name": "METEOR",
        "purpose": "Evaluates summary quality.",
        "application": "Used for summarization task evaluation.",
        "evidence": "For summarization, we also employ the 'METEOR' metric, which offers a more nuanced evaluation of the summary quality."
      },
      {
        "name": "F1, Recall, Precision",
        "purpose": "Measures NER performance.",
        "application": "Used to evaluate NER task performance.",
        "evidence": "Finally, for the NER task, we employ the standard F1, recall, and precision metrics."
      }
    ]
  },
  "method_type": {
    "methods": [
      {
        "name": "Transformation/structurization of user input",
        "description": "The system transforms scientific text into structured summaries, answers, and recognized entities.",
        "evidence": "We employ pre-trained models and fine-tune them on datasets in the scientific domain. The models are adapted for four key Natural Language Processing (NLP) tasks: summarization, text generation, question answering, and named entity recognition."
      },
      {
        "name": "Knowledge Extraction and Structurization",
        "description": "The system extracts and structures knowledge from scientific texts.",
        "evidence": "Our results indicate that domain-specific fine-tuning significantly enhances model performance in each of these tasks, thereby improving their applicability for scientific contexts."
      }
    ]
  },
  "subject_area": {
    "areas": [
      {
        "name": "Interdisciplinary Sciences",
        "description": "The paper develops a framework for extracting and understanding scientific knowledge across various domains.",
        "evidence": "This project investigates the efficacy of Large Language Models (LLMs) in understanding and extracting scientific knowledge across specific domains."
      }
    ]
  },
  "performance_summary": {
    "performance_summary": [
      {
        "summary": "Fine-tuned models show improved performance in summarization, text generation, question answering, and named entity recognition tasks.",
        "evidence": "Our results indicate that domain-specific fine-tuning significantly enhances model performance in each of these tasks."
      }
    ],
    "baselines": [
      {
        "name": "Baseline",
        "description": "Initial model performance before fine-tuning.",
        "evidence": "Baseline performance is compared to fine-tuned models in various tasks."
      }
    ],
    "benchmark_datasets": [
      {
        "name": "Arxiv",
        "data_description": "A large dataset of scientific articles.",
        "usage": "Used for training and evaluation in summarization tasks.",
        "evidence": "In addressing the challenge of training with limited computational resources on the extensive Arxiv dataset, comprising 1.7 million articles, we sampled 2% (around 34,000 articles) to manage a representative subset effectively."
      }
    ],
    "evaluation_metrics": [
      {
        "name": "ROUGE",
        "purpose": "Measures overlap between generated and reference texts.",
        "application": "Used to assess text generation and summarization tasks.",
        "evidence": "We employ different metrics to assess the finetuning and model performance on the respective tasks. For text generation, summarization, and Q & A, we measure the various ROUGE scores."
      },
      {
        "name": "METEOR",
        "purpose": "Evaluates summary quality.",
        "application": "Used for summarization task evaluation.",
        "evidence": "For summarization, we also employ the 'METEOR' metric, which offers a more nuanced evaluation of the summary quality."
      },
      {
        "name": "F1, Recall, Precision",
        "purpose": "Measures NER performance.",
        "application": "Used to evaluate NER task performance.",
        "evidence": "Finally, for the NER task, we employ the standard F1, recall, and precision metrics."
      }
    ]
  },
  "benchmark_dataset": {
    "name": "SQUAD",
    "data_description": "Dataset for extractive question answering.",
    "usage": "Used for fine-tuning models for extractive QA.",
    "evidence": "Question answering fine-tunes two sets of models, BERT and SciBERT are fine-tuned on SQUAD for Extractive."
  },
  "limitations": {
    "limitations": [
      {
        "name": "Limited Computational Resources",
        "description": "The study faced challenges in training with limited computational resources on large datasets.",
        "evidence": "In addressing the challenge of training with limited computational resources on the extensive Arxiv dataset, comprising 1.7 million articles, we sampled 2% (around 34,000 articles) to manage a representative subset effectively."
      },
      {
        "name": "Model Input Constraints",
        "description": "BART's maximum input token constraint limited its ability to process longer documents effectively.",
        "evidence": "However, we encountered significant limitations related to BART’s maximum input token constraint, which hindered its ability to process longer documents effectively."
      }
    ]
  },
  "future_directions": {
    "future_directions": [
      {
        "name": "Optimize Data Handling",
        "description": "Enhance efficiency by optimizing data handling and computational pathways in adapted layers.",
        "evidence": "Future efforts could enhance efficiency by optimizing data handling, computational pathways in adapted layers, and improving hardware utilization, aiming to maximize the gains from LoRA adaptations."
      },
      {
        "name": "Explore Alternative Models",
        "description": "Investigate alternative models designed to handle extensive texts.",
        "evidence": "This challenge compelled us to explore alternative models designed to handle extensive texts."
      }
    ]
  },
  "resource_link": {
    "answer": "https://github.gatech.edu/cs7643Team1/cs7643_project",
    "evidence": "The code used to deploy these models and fine-tune them are stored in the Georgia Tech Github organization here: [1]."
  }
}