{
  "objective": {
    "answer": "The primary objective of the paper is to address the limitations of standard LLM listwise reranking in scientific retrieval by proposing CORANK, a training-free, model-agnostic reranking framework that uses compact document representations based on semantic features to improve reranking performance.",
    "evidence": "To address these challenges, we explore compact document representations based on semantic features (e.g., categories, sections and keywords) and propose CORANK, a training-free, model-agnostic reranking framework for scientific retrieval."
  },
  "knowledge_gap": {
    "answer": "The paper addresses the gap in effective reranking methods for scientific retrieval, where standard LLM listwise reranking is limited by suboptimal first-stage retrieval and the token overhead of full-text representations.",
    "evidence": "In the scientific domain, retrievers often show limited performance, so truly relevant documents may not rank high enough in the first-stage retrieval. Moreover, due to the substantial token overhead of full text representation and finite context length, rerankers can only operate on a limited number of candidates."
  },
  "novelty": {
    "answer": [
      "The introduction of CORANK, a training-free, model-agnostic reranking framework that uses compact semantic features for reranking.",
      "The use of semantic features such as categories, sections, and keywords as compact document representations for reranking.",
      "A hybrid reranking strategy that combines coarse reranking with compact features and fine-grained reranking with full-text inputs."
    ],
    "evidence": [
      "Motivated by these insights, we propose CORANK, a training-free, model-agnostic reranking framework for science retrieval.",
      "Instead of using full text, we investigate an information extraction (IE)â€“based approach, representing each scientific paper using high-level features such as categories, sections, and keywords.",
      "Our design enhances the robustness against suboptimal first-stage retrieval with compact feature representation and maximizes the overall effectiveness with final full text reranking."
    ]
  },
  "inspirational_papers": {
    "answer": "- Sun et al. (2023a) The standard practice in LLM listwise reranking inspired the exploration of compact document representations. (Methodological precursors)\n- Pradeep et al. (2023a) Their work on LLM-based listwise rerankers influenced the development of CORANK. (Methodological precursors)",
    "evidence": "The standard practice in LLM listwise reranking (Sun et al., 2023a; Pradeep et al., 2023a,b; Gangi Reddy et al., 2024; Liu et al., 2024c) is to input the full text of each candidate document into the context window."
  },
  "method": {
    "steps": [
      {
        "step": "Offline extraction of structured semantic features from scientific documents.",
        "input": "Unstructured scientific documents.",
        "output": "High-level semantic features like categories, sections, and keywords.",
        "evidence": "To construct compact, feature-based representations for each scientific document, we first perform document-level information extraction offline."
      },
      {
        "step": "Coarse-grained reranking using compact semantic features.",
        "input": "Compact semantic representations of documents and a query.",
        "output": "A high-quality subset of top candidate documents.",
        "evidence": "We then replace each document with a compact semantic representation and apply LLM-based listwise reranking to identify a high-quality subset."
      },
      {
        "step": "Fine-grained reranking using full-text inputs.",
        "input": "Full text of the top candidate documents from coarse reranking.",
        "output": "Final ranking of documents based on detailed relevance.",
        "evidence": "In the second reranking stage, we refine the ranking over the seed set of candidates using full documents."
      }
    ],
    "tools": [
      {
        "name": "Qwen3-8B-Instruct",
        "description": "Used for zero-shot information extraction to obtain semantic features.",
        "evidence": "For semantic feature extraction, we find that small open-source models are sufficiently effective; we therefore adopt Qwen3-8B-Instruct for all extraction tasks."
      },
      {
        "name": "Contriever",
        "description": "Used as the first-stage retriever and for semantic similarity scoring for filtering features.",
        "evidence": "Following Gangi Reddy et al. (2024), we use Contriever as the first-stage retriever and the similarity encoder for adaptive selection."
      }
    ],
    "benchmark_datasets": [
      {
        "name": "LitSearch",
        "data_description": "A benchmark of expert-annotated complex literature queries targeting recent ML and NLP papers.",
        "usage": "Used to evaluate different reranking methods.",
        "evidence": "We evaluate different reranking methods on two high-quality scientific retrieval benchmarks: LitSearch and CSFCube."
      },
      {
        "name": "CSFCube",
        "data_description": "A human-annotated testbed for faceted query-by-example retrieval.",
        "usage": "Used to evaluate different reranking methods.",
        "evidence": "We evaluate different reranking methods on two high-quality scientific retrieval benchmarks: LitSearch and CSFCube."
      }
    ],
    "evaluation_metrics": [
      {
        "name": "nDCG",
        "purpose": "Measures the quality of document ranking based on relevance.",
        "application": "Used to evaluate the effectiveness of reranking methods.",
        "evidence": "The quality is often evaluated using top-k metrics such as nDCG@10, with small k values that stress accuracy at the top of the ranked list."
      },
      {
        "name": "MAP",
        "purpose": "Measures the mean average precision of the ranked documents.",
        "application": "Used to evaluate the effectiveness of reranking methods.",
        "evidence": "We include nDCG, MAP, and Recall as our evaluation metrics."
      },
      {
        "name": "Recall",
        "purpose": "Measures the ability to retrieve all relevant documents.",
        "application": "Used to evaluate the effectiveness of reranking methods.",
        "evidence": "We include nDCG, MAP, and Recall as our evaluation metrics."
      }
    ]
  },
  "method_type": {
    "methods": [
      {
        "name": "Knowledge Extraction and Structurization",
        "description": "The approach involves extracting and structuring knowledge from unstructured scientific documents.",
        "evidence": "To construct compact, feature-based representations for each scientific document, we first perform document-level information extraction offline."
      },
      {
        "name": "Iterative refinement of Ideas, Hypothesis and Experiment design",
        "description": "The method includes iterative refinement of document ranking through coarse and fine-grained reranking stages.",
        "evidence": "The coarse reranking stage expands the reranking pool, improving robustness to suboptimal first-stage retrieval in the scientific domain. The fine-grained reranking stage, in turn, preserves sufficient detail for precise relevance comparisons."
      }
    ]
  },
  "subject_area": {
    "areas": [
      {
        "name": "Interdisciplinary Sciences",
        "description": "The paper addresses challenges in scientific retrieval, which is crucial for advancing academic discovery across multiple disciplines.",
        "evidence": "Scientific retrieval is essential for advancing academic discovery. Within this process, document reranking plays a critical role by refining first-stage retrieval results."
      },
      {
        "name": "Applied Sciences & Engineering",
        "description": "The proposed method improves document retrieval systems, which are applied in various scientific and engineering domains.",
        "evidence": "Our design enhances the robustness against suboptimal first-stage retrieval with compact feature representation and maximizes the overall effectiveness with final full text reranking."
      }
    ]
  },
  "performance_summary": {
    "performance_summary": [
      {
        "summary": "CORANK significantly improves reranking performance, achieving an absolute improvement of +11.5 nDCG@10 without the sliding window strategy.",
        "evidence": "Empirical experiments show that across various LLM backbones, CORANK achieves an absolute improvement of +11.5 nDCG@10 without the sliding window strategy, and retains a +3.9 gain with it."
      }
    ],
    "baselines": [
      {
        "name": "RankVicuna",
        "description": "A supervised, model-specific reranking model trained on large-scale general-domain datasets.",
        "evidence": "For supervised, model-specific baselines, we compare against RankVicuna."
      },
      {
        "name": "RankZephyr",
        "description": "A supervised, model-specific reranking model trained on large-scale general-domain datasets.",
        "evidence": "For supervised, model-specific baselines, we compare against RankZephyr."
      }
    ],
    "benchmark_datasets": [
      {
        "name": "LitSearch",
        "data_description": "A benchmark of expert-annotated complex literature queries targeting recent ML and NLP papers.",
        "usage": "Used to evaluate different reranking methods.",
        "evidence": "We evaluate different reranking methods on two high-quality scientific retrieval benchmarks: LitSearch and CSFCube."
      },
      {
        "name": "CSFCube",
        "data_description": "A human-annotated testbed for faceted query-by-example retrieval.",
        "usage": "Used to evaluate different reranking methods.",
        "evidence": "We evaluate different reranking methods on two high-quality scientific retrieval benchmarks: LitSearch and CSFCube."
      }
    ],
    "evaluation_metrics": [
      {
        "name": "nDCG",
        "purpose": "Measures the quality of document ranking based on relevance.",
        "application": "Used to evaluate the effectiveness of reranking methods.",
        "evidence": "The quality is often evaluated using top-k metrics such as nDCG@10, with small k values that stress accuracy at the top of the ranked list."
      },
      {
        "name": "MAP",
        "purpose": "Measures the mean average precision of the ranked documents.",
        "application": "Used to evaluate the effectiveness of reranking methods.",
        "evidence": "We include nDCG, MAP, and Recall as our evaluation metrics."
      },
      {
        "name": "Recall",
        "purpose": "Measures the ability to retrieve all relevant documents.",
        "application": "Used to evaluate the effectiveness of reranking methods.",
        "evidence": "We include nDCG, MAP, and Recall as our evaluation metrics."
      }
    ]
  },
  "benchmark_dataset": null,
  "limitations": {
    "limitations": [
      {
        "name": "Feature Extraction Quality",
        "description": "The zero-shot information extraction approach used for feature extraction is relatively simple and may have room for improvement.",
        "evidence": "In our offline preprocessing stage, we apply zero-shot information extraction using LLMs to obtain semantic features such as categories for each document. This is a relatively simple approach."
      },
      {
        "name": "Applicability Across Domains",
        "description": "The method is specifically tailored for scientific retrieval and has not been tested in general-domain settings.",
        "evidence": "This work focuses exclusively on scientific retrieval. The motivation and assumptions behind our method are specifically tailored to the characteristics of this domain."
      }
    ]
  },
  "future_directions": {
    "future_directions": [
      {
        "name": "Improving Feature Extraction",
        "description": "Explore using more specialized models or introducing multi-turn feedback mechanisms to enhance the quality of extracted features.",
        "evidence": "While it has proven effective based on both quantitative results and case studies, there is still potential room for improvement. For instance, one could explore using more specialized models or introducing multi-turn feedback mechanisms to enhance the quality of extracted features."
      },
      {
        "name": "Extending to General Domains",
        "description": "Investigate the applicability of the framework in general-domain retrieval settings.",
        "evidence": "An important direction for future work is to investigate the applicability of our framework in general-domain retrieval settings."
      }
    ]
  },
  "resource_link": {
    "answer": "",
    "evidence": "Code will be publicly available."
  }
}