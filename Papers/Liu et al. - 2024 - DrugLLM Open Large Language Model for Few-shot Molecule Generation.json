{
  "objective": {
    "answer": "The primary objective of the paper is to develop DrugLLM, a large language model specifically tailored for drug design that can perform few-shot molecule generation and optimization. The authors aim to address the challenge of generating new molecules with desired properties based on limited examples, by capturing the relationship between molecular structure and pharmacochemical properties. DrugLLM is trained to predict molecular modifications that enhance specific properties, enabling efficient few-shot learning in drug discovery.",
    "evidence": "In this work, we introduced DrugLLM, a LLM tailored for drug design. During the training process, we employed Group-based Molecular Representation (GMR) to represent molecules, arranging them in sequences that reflect modifications aimed at enhancing specific molecular properties. DrugLLM learns how to modify molecules in drug discovery by predicting the next molecule based on past modifications. Extensive computational experiments demonstrate that DrugLLM can generate new molecules with expected properties based on limited examples, presenting a powerful few-shot molecule generation capacity."
  },
  "knowledge_gap": {
    "answer": "Current large language models and deep learning methods are inadequate at handling the languages of biology and chemistry, particularly in capturing the relationship between molecular structure and properties, which impedes few-shot molecule generation for drug design.",
    "evidence": "Despite the emergence of diverse techniques to improve few-shot learning capacity, current LLMs fall short in handling the languages in biology and chemistry. For example, they are struggling to capture the relationship between molecule structure and pharmacochemical properties. Consequently, the few-shot learning capacity of small-molecule drug modification remains impeded."
  },
  "novelty": {
    "answer": [
      "Introduction of DrugLLM, the first large language model specifically designed for few-shot molecule generation and optimization in drug discovery.",
      "Development and use of Group-based Molecular Representation (GMR) to represent molecules, addressing token abundance, cyclic complexity, and structural sensitivity issues inherent in SMILES notation.",
      "A novel training methodology that organizes molecular modification sequences as paragraphs, each focused on a specific property, enabling in-context few-shot learning.",
      "Demonstration that DrugLLM can perform both few-shot and zero-shot molecule optimization, outperforming existing large language models and molecule generators."
    ],
    "evidence": [
      "To the best of our knowledge, DrugLLM is the first large language model for few-shot molecule generation.",
      "In DrugLLM, molecules are represented using Group-based Molecular Representation (GMR), which is a novel type of molecular representation to address token abundance, cyclic complexity, and structural sensitivity inherent in SMILES.",
      "The methodology organizes the modification sequences in accordance with specific molecular properties. Drawing an analogy, a case of molecule modification (a pair of molecules with similar structures) toward a certain property serves as a 'sentence'. Multiple cases of modification toward the identical property constitute a paragraph.",
      "It is noteworthy that DrugLLM is one of the very few approaches that support zero-shot molecule optimization... In terms of DrugLLM, it improves the optimization success rates by significant margins compared with the other LLM, indicating a better capacity in instruction understanding and molecule optimization."
    ]
  },
  "inspirational_papers": {
    "answer": [
      "- Jin et al. (2018) Junction tree variational autoencoder for molecular graph generation. (Experimental baselines)",
      "- Jin et al. (2020) Hierarchical generation of molecular graphs using structural motifs. (Experimental baselines)",
      "- Maziarz et al. (2021) Learning to extend molecular scaffolds with structural motifs. (Experimental baselines)",
      "- Popova et al. (2018) Deep reinforcement learning for de novo drug design. (Methodological precursor)",
      "- Ross et al. (2022) Large-scale chemical language representations capture molecular structure and properties. (Papers with limitations addressed by this work)"
    ],
    "evidence": [
      "For comparison, we take the junction tree-based variational auto-encoder (JTVAE) (Jin et al., 2018), the variational junction tree neural network (VJTNN) (Jin et al., 2020), and the scaffold-based molecule generator (MoLeR) (Maziarz et al., 2021).",
      "Popova et al. (2018) integrates both generative and predictive neural networks to generate novel targeted chemical libraries in a trial-and-error manner.",
      "Ross et al. (2022). Large-scale chemical language representations capture molecular structure and properties, Nature Machine Intelligence 4(12): 1256–1264."
    ]
  },
  "method": {
    "steps": [
      {
        "step": "Data Collection and Preparation",
        "input": "Molecular data from ZINC and ChEMBL databases, filtered for drug-like molecules and labeled with properties.",
        "output": "Large-scale dataset of molecules and their properties, organized in tabular form.",
        "evidence": "To train and analyze the DrugLLM model, we construct a large-scale dataset from the ZINC (Wu et al., 2018) and ChEMBL (Davies et al., 2015; Mendez et al., 2019) datasets."
      },
      {
        "step": "Transformation to Textual Sentences and Paragraphs",
        "input": "Tabular molecular data with properties.",
        "output": "Textual sentences (pairs of similar molecules with modifications) and paragraphs (sequences of modifications focused on a property).",
        "evidence": "Based on the collected tabular data, we then transform them into meaningful textual sentences and paragraphs. In particular, we regard the modification between two molecules with similar structures as a sentence and multiple cases of molecular modifications as a paragraph."
      },
      {
        "step": "Group-based Molecular Representation (GMR) Construction",
        "input": "SMILES strings of molecules.",
        "output": "GMR strings encoding structural groups and their connections.",
        "evidence": "GMR begins by assigning a unique string identifier to each structural group, thereby constructing a comprehensive dictionary. When a molecule is processed within the GMR framework, its SMILES string is converted into an encoded string that encapsulates both the identity of the structural groups and their positional relationships within the molecule."
      },
      {
        "step": "Model Training",
        "input": "Paragraphs of molecular modifications in GMR format, with property optimization descriptions.",
        "output": "Trained DrugLLM model capable of predicting molecular modifications and generating new molecules.",
        "evidence": "We train DrugLLM using the AdamW optimizer for six weeks on eight NVIDIA RTX 3090 GPUs, where it learns to generate the paragraphs from scratch."
      },
      {
        "step": "Few-shot and Zero-shot Molecule Generation",
        "input": "For few-shot: K pairs of example modifications and a benchmark molecule. For zero-shot: Natural language instruction and a molecule.",
        "output": "Generated molecules with optimized properties.",
        "evidence": "The objective of the model is to generate a new molecule that not only maintains structural similarity to the benchmark molecule but also exhibits superior properties (either increased or decreased, as guided by the examples)... we explore zero-shot molecule optimization, which involves generating modified molecules with better properties of interest according to natural language instruction without any specific training instances."
      }
    ],
    "tools": [
      "DrugLLM: A large language model based on the Transformer architecture (LLaMA 7B), trained for molecule generation and optimization.",
      "GMR (Group-based Molecular Representation): A molecular encoding scheme that represents molecules as sequences of structural groups.",
      "AdamW optimizer: Used for model training.",
      "RDKit: Used for calculating molecular properties and evaluating generated molecules.",
      "Message-passing neural network: Used to predict biological activities for evaluation.",
      "UMAP: Used for visualizing chemical space similarity."
    ],
    "evidence": [
      "Following recent work on the pre-training large language models, DrugLLM is based on the Transformer architecture (Vaswani et al., 2017). We adopt the parameters of LLama 7B (Touvron et al., 2023) and expand the vocabulary by introducing the frequently-used SMILES tokens.",
      "The core of the GMR framework involves decomposing molecules into structural groups and noting the inter-group connections, facilitating the group-based reconstruction of SMILES strings.",
      "The training process employs the AdamW optimizer with a learning rate of 3 × 10−5, and we adopt a cosine annealing schedule to adjust the learning rate.",
      "The generated molecules are evaluated by the Python scripts via the RDKit library.",
      "Instead, we leverage a message-passing neural network to predict biological activities.",
      "To visually represent the structural similarity between the benchmark and generated molecules in the chemical space, we employ the Uniform Manifold Approximation and Projection (UMAP) method to create a chart (Figure 2B)."
    ]
  },
  "subject_area": {
    "areas": [
      "Chemical Sciences",
      "Biological Sciences",
      "Health Sciences"
    ],
    "evidence": [
      "Small molecules play a critical role in drug discovery due to their ability to bind to specific biological targets and modulate their functions...",
      "DrugLLM, a LLM tailored for drug design...",
      "The molecules produced by DrugLLM are usually novel and are not recorded in the ChEMBL database. Unlike the physicochemical properties mentioned above, the biological activities of molecules (e.g. the Ki value on streptokinase A) are difficult to estimate by chemical or physical rules."
    ]
  },
  "performance_summary": {
    "performance_summary": [
      "DrugLLM significantly outperforms state-of-the-art molecule generators (JTVAE, VJTNN, MoLeR) and large language models (ChatGPT, GPT-4, ChatGLM) in few-shot and zero-shot molecule optimization tasks.",
      "DrugLLM achieves a success rate of up to 75% in few-shot molecule generation for physicochemical properties, compared to about 50% for baselines.",
      "For biological activity optimization, DrugLLM attains an average success rate of 0.68, outperforming all baselines (which range from 0.46 to 0.54).",
      "In zero-shot molecule optimization, DrugLLM achieves higher success rates (up to 0.61) than ChatGPT, GPT-4, and ChatGLM."
    ],
    "baselines": [
      "JTVAE: Junction tree-based variational autoencoder for molecular graph generation.",
      "VJTNN: Variational junction tree neural network for molecular graph generation.",
      "MoLeR: Scaffold-based molecule generator using structural motifs.",
      "Random generation: Random sampling based on the latent space of JTVAE.",
      "ChatGLM, ChatGPT, GPT-4: Large language models used for zero-shot molecule optimization comparison."
    ],
    "benchmark_datasets": [
      "ZINC: Contains more than 230 million purchasable compounds in ready-to-dock, 3D formats. Used for training and constructing modification paragraphs.",
      "ChEMBL: Comprehensive repository for bioactive compounds with their properties. Used for training, constructing modification paragraphs, and evaluating biological activity optimization."
    ],
    "evaluation_metrics": [
      "Success rate: Proportion of generated molecules that adhere to the rule of examples of modifications.",
      "Molecular similarity: Measures how similar the generated molecule is to the benchmark molecule.",
      "Pearson correlation (for biological activity prediction): Measures the correlation between predicted and real biological activity values.",
      "Kernel Density Estimation (KDE): Used to visualize the distribution of molecular properties.",
      "UMAP: Used to visualize chemical space similarity."
    ],
    "evidence": [
      "We note that the three baseline molecule generators, namely JTVAE, VJTNN, and MoLeR, just obtained a success rate of about 50%, which is similar to a random generation. In contrast, DrugLLM exhibits a progressive improvement in few-shot molecule generation, with the accuracy of the generated molecules increasing incrementally to 75% as the number of shots increases.",
      "We observe that the three generator baselines fail to obtain meaningful improvement compared with the random generation (Table 2), indicating that these molecule generators still struggle to capture the modification rules underlying the limited examples. As for DrugLLM, it significantly outperforms the other baselines by a large margin in most of the test properties.",
      "It is noteworthy that DrugLLM is one of the very few approaches that support zero-shot molecule optimization... In terms of DrugLLM, it improves the optimization success rates by significant margins compared with the other LLM, indicating a better capacity in instruction understanding and molecule optimization.",
      "The quality of the generated molecules was assessed based on their success rate and molecular similarity.",
      "The generated molecules are evaluated by the Python scripts via the RDKit library.",
      "ZINC is a free database that contains more than 230 million purchasable compounds in ready-to-dock, 3D formats. We filter the druglike molecules from ZINC and obtain 4.5 million molecules. ChEMBL is a comprehensive repository for bioactive compounds with their properties."
    ]
  },
  "limitations": {
    "limitations": [
      {
        "label": "Limited Number of Shots",
        "explanation": "DrugLLM currently supports up to 9 shots of molecule modifications due to hardware limitations, restricting its few-shot learning capacity.",
        "evidence": "Firstly, DrugLLM merely supports up to 9 shots of molecule modifications due to the limitation of the hardware."
      },
      {
        "label": "Elementary Zero-shot Optimization",
        "explanation": "The zero-shot molecule optimization capability of DrugLLM is basic and limited to compositions of two known molecular properties.",
        "evidence": "Secondly, the zero-shot molecule optimization of DrugLLM is relatively elementary. The current DrugLLM is only capable of optimizing the molecules toward the composition of two known molecular properties. The zero-shot learning ability for arbitrary instructions is still lagging behind."
      },
      {
        "label": "GMR Representation Limitations",
        "explanation": "The current GMR has difficulty representing a small number of complex molecules and lacks certain standardization measures.",
        "evidence": "Thirdly, the GMR currently in use has difficulty representing a small number of complex molecules under certain circumstances and lacks certain standardization measures."
      }
    ],
    "evidence": [
      "Firstly, DrugLLM merely supports up to 9 shots of molecule modifications due to the limitation of the hardware.",
      "Secondly, the zero-shot molecule optimization of DrugLLM is relatively elementary. The current DrugLLM is only capable of optimizing the molecules toward the composition of two known molecular properties. The zero-shot learning ability for arbitrary instructions is still lagging behind.",
      "Thirdly, the GMR currently in use has difficulty representing a small number of complex molecules under certain circumstances and lacks certain standardization measures."
    ]
  },
  "future_directions": {
    "future_directions": [
      "Increase Input Length: The authors plan to increase the input length (number of shots) to achieve more impressive performance in drug design.",
      "Enhance Zero-shot Learning: They aim to improve the zero-shot molecule optimization ability for arbitrary instructions.",
      "Optimize GMR Representation: The authors intend to optimize GMR for special cases and add standardization methods to reduce encoding errors."
    ],
    "evidence": [
      "In the future, we will increase the input length (i.e., the number of shots) to achieve more impressive performance in drug design.",
      "The zero-shot learning ability for arbitrary instructions is still lagging behind.",
      "In the future, we will optimize for special cases of GMR representation and add standardization methods to reduce the small number of encoding errors that the model may generate."
    ]
  },
  "resource_link": {
    "answer": "",
    "evidence": "No code repository, project website, or data repository link is provided in the paper."
  },
  "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
  "authors": [
    "Xianggen",
    "Yan",
    "Haoran",
    "Jin",
    "Shudong",
    "Bowen",
    "Jiancheng"
  ],
  "published": "2024-05-07",
  "link": "http://arxiv.org/abs/2405.06690"
}