{
  "objective": {
    "answer": "The primary objective of the paper is to enhance the Chinese language ability and scientific reasoning ability of the Llama-3 model through continual pre-training, while retaining its original capabilities.",
    "evidence": "Our focus is to enhance the model’s capacities from two major aspects: Chinese language ability and scientific reasoning ability, while retaining its original capabilities."
  },
  "knowledge_gap": {
    "answer": "The paper addresses the gap in effectively enhancing specific abilities of large language models, such as Chinese language and scientific reasoning, without causing catastrophic forgetting of their original capabilities.",
    "evidence": "Although CPT has been widely conducted in existing work, the key training details (e.g., data selection, mixture, and curriculum) to develop new abilities and maintain existing abilities have not been well discussed, especially how to boost the comprehensive capacities of a well-trained model under a limited training budget."
  },
  "novelty": {
    "answer": [
      "The paper introduces a novel data synthesis technique to generate high-quality scientific and code data in the form of QA pairs.",
      "The study presents a unique approach to data curation, including topic-based data mixture and perplexity-based data curriculum strategies.",
      "The work is the first public report on synthesizing and utilizing large-scale multidisciplinary scientific data for continual pre-training."
    ],
    "evidence": [
      "We extensively explore the data synthesis technique, and generate high-quality scientific and code data (in the format of QA pairs).",
      "We design specific data curation strategies to improve the backbone models. For Chinese language ability, we collect and select extensive Chinese text data from diverse sources for effective bilingual adaptation.",
      "To the best of our knowledge, it is the first public work that reports how to synthesize and utilize large-scale multidisciplinary scientific data for continual pre-training."
    ]
  },
  "inspirational_papers": {
    "answer": "- Ke et al. (2023) Continual pre-training of language models inspired our approach to enhance domain-specific abilities. (Methodological precursors)\n- Luo et al. (2023) Their work on catastrophic forgetting informed our strategies to retain original model capabilities. (Papers with limitations addressed by this work)",
    "evidence": "To address these issues, a widely-used approach is to conduct continual pre-training (CPT) for LLMs on specially-curated data related to the expected abilities (Ke et al., 2023; Gupta et al., 2023; Ibrahim et al., 2024). During the CPT process, catastrophic forgetting (Luo et al., 2023) has become a common technical issue."
  },
  "method": {
    "steps": [
      {
        "step": "Bilingual adaptation stage to improve Chinese language capabilities.",
        "input": "Chinese and English corpora with a 2:8 ratio.",
        "output": "Enhanced Chinese language understanding in Llama-3.",
        "evidence": "Following our previous experiences with Yulan-3 (Zhu et al., 2024), we set the ratio of Chinese and English corpora as 2:8, to balance the Chinese and English capabilities."
      },
      {
        "step": "Synthetic enhancement stage to improve scientific reasoning.",
        "input": "Synthetic data in the form of QA pairs from scientific disciplines.",
        "output": "Improved scientific reasoning capabilities in Llama-3.",
        "evidence": "We generate synthetic data in the format of the question and answer (QA) pair, to cover a broad spectrum of multidisciplinary scientific knowledge."
      }
    ],
    "tools": [
      {
        "name": "GPT-4",
        "description": "Used to annotate web pages for topic classification.",
        "evidence": "Furthermore, we employ GPT-4 to annotate a small number of web pages as training data for our topic classifiers."
      },
      {
        "name": "Mistral-7B-Instruct-v0.3",
        "description": "Used to generate synthetic QA pairs.",
        "evidence": "Then, we utilize Mistral-7B-Instruct-v0.3 to generate relevant QA pairs that align with the targeted scientific discipline."
      }
    ],
    "benchmark_datasets": [
      {
        "name": "C-Eval",
        "data_description": "Chinese language understanding benchmark.",
        "usage": "Used to evaluate the Chinese language understanding capability.",
        "evidence": "We evaluate the English language understanding capability using the MMLU (Hendrycks et al., 2021a), and select CMMLU (Li et al., 2023) and C-Eval (Huang et al., 2023) for evaluating Chinese language understanding capability."
      },
      {
        "name": "MATH",
        "data_description": "Mathematical reasoning benchmark.",
        "usage": "Used to evaluate mathematical reasoning capabilities.",
        "evidence": "Specifically, we find that synthetic data is very useful to enhance the capacities of LLMs in scientific knowledge reasoning."
      }
    ],
    "evaluation_metrics": [
      {
        "name": "Perplexity (PPL)",
        "purpose": "Measures the model's capability regarding specific topics.",
        "application": "Used to track the LLM’s capabilities on different topic categories during the training process.",
        "evidence": "To track the LLM’s capabilities on different topic categories during the training process, we evaluate the change of the perplexity (PPL) score in each topic on the validation set."
      }
    ]
  },
  "method_type": {
    "methods": [
      {
        "name": "Hypothesis or Idea Generation",
        "description": "The system produces candidate hypotheses or new research ideas from prior knowledge or external input.",
        "evidence": "We extensively explore data synthesis technique in continual pre-training, and generate multidisciplinary scientific QA data."
      },
      {
        "name": "Experimental design generation",
        "description": "The approach includes producing experimental protocols, configurations, or evaluation strategies.",
        "evidence": "We conduct comprehensive experiments comparing Llama-3-SynE with other competitive LLMs across various evaluation benchmarks."
      }
    ]
  },
  "subject_area": {
    "areas": [
      {
        "name": "Interdisciplinary Sciences",
        "description": "The paper focuses on enhancing language models for both Chinese language and scientific reasoning tasks.",
        "evidence": "Our focus is to enhance the model’s capacities from two major aspects: Chinese language ability and scientific reasoning ability."
      },
      {
        "name": "Applied Sciences & Engineering",
        "description": "The methodology involves synthesizing scientific QA data to improve reasoning capabilities.",
        "evidence": "We generate synthetic data in the format of the question and answer (QA) pair, to cover a broad spectrum of multidisciplinary scientific knowledge."
      }
    ]
  },
  "performance_summary": {
    "performance_summary": [
      {
        "summary": "Llama-3-SynE significantly improved Chinese language understanding and scientific reasoning abilities without degrading original capabilities.",
        "evidence": "Extensive experiments show that our CPT approach is very effective (yielding large improvements on Chinese and scientific benchmarks without hurting the performance on English benchmarks)."
      }
    ],
    "baselines": [
      {
        "name": "Llama-3 (8B)",
        "description": "The original backbone model used for comparison.",
        "evidence": "Our focus is to enhance the model’s capacities from two major aspects: Chinese language ability and scientific reasoning ability, while retaining its original capabilities."
      },
      {
        "name": "MAmmoTH2-8B",
        "description": "A scientific LLM baseline for comparison.",
        "evidence": "Among all the baselines, MAmmoTH2-8B achieves very good performance on English scientific benchmarks."
      }
    ],
    "benchmark_datasets": [
      {
        "name": "C-Eval",
        "data_description": "Chinese language understanding benchmark.",
        "usage": "Used to evaluate the Chinese language understanding capability.",
        "evidence": "We evaluate the English language understanding capability using the MMLU (Hendrycks et al., 2021a), and select CMMLU (Li et al., 2023) and C-Eval (Huang et al., 2023) for evaluating Chinese language understanding capability."
      },
      {
        "name": "MATH",
        "data_description": "Mathematical reasoning benchmark.",
        "usage": "Used to evaluate mathematical reasoning capabilities.",
        "evidence": "Specifically, we find that synthetic data is very useful to enhance the capacities of LLMs in scientific knowledge reasoning."
      }
    ],
    "evaluation_metrics": [
      {
        "name": "Perplexity (PPL)",
        "purpose": "Measures the model's capability regarding specific topics.",
        "application": "Used to track the LLM’s capabilities on different topic categories during the training process.",
        "evidence": "To track the LLM’s capabilities on different topic categories during the training process, we evaluate the change of the perplexity (PPL) score in each topic on the validation set."
      }
    ]
  },
  "benchmark_dataset": {
    "name": null,
    "description": null,
    "usage": null,
    "evidence": "No traditional benchmark dataset was used; the study primarily utilized author-collected and synthesized datasets."
  },
  "limitations": {
    "limitations": [
      {
        "name": "Limited Generalizability",
        "description": "The findings from the surrogate model TinyLlama may not fully generalize to larger models like Llama-3.",
        "evidence": "Due to the significant costs involved in tuning experiments on Llama-3 (8B), we use a relatively small model TinyLlama as a surrogate model for extensive exploratory experiments."
      }
    ]
  },
  "future_directions": {
    "future_directions": [
      {
        "name": "Expand Synthetic Data Coverage",
        "description": "Plan to expand the list of domain names for scientific disciplines to enhance coverage.",
        "evidence": "For each discipline, we manually collect a list of domain names relevant to the respective fields, such as math.stackexchange.com and physicsforums.com, allowing for the expansion of this list as needed to enhance the coverage."
      }
    ]
  },
  "resource_link": {
    "answer": "https://github.com/RUC-GSAI/Llama-3-SynE",
    "evidence": "Our model, data, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE."
  }
}