{
  "objective": {
    "answer": "The primary objective of the paper is to introduce Tree-of-Debate (ToD), a framework that converts scientific papers into LLM personas that debate their respective novelties, enabling fine-grained analysis of independent novelty arguments within scholarly articles.",
    "evidence": "Inspired by this, we introduce Tree-of-Debate (ToD), a framework which converts scientific papers into LLM personas that debate their respective novelties."
  },
  "knowledge_gap": {
    "answer": "The paper addresses the challenge of identifying novel findings and discerning distinctions between related papers, especially those presenting similar ideas from different angles, which is difficult due to the overwhelming volume of information in open-access repositories.",
    "evidence": "This astronomical surge in scholarly articles makes it difficult to identify novel findings and discern the distinctions between related papers, especially those presenting similar ideas from different angles."
  },
  "novelty": {
    "answer": [
      "Introduction of a structured multi-persona debate framework, Tree-of-Debate, to generate fine-grained contrastive summaries.",
      "Dynamic construction of a debate tree to reason about fine-grained arguments discussed in scholarly articles.",
      "Integration of iterative retrieval throughout a debate to improve fine-grained reasoning."
    ],
    "evidence": [
      "We introduce Tree-of-Debate, a structured multi-persona debate framework, to generate fine-grained contrastive summaries.",
      "Tree-of-Debate can dynamically construct a debate tree to reason about fine-grained arguments discussed in scholarly articles.",
      "Iterative retrieval throughout a debate improves fine-grained reasoning."
    ]
  },
  "inspirational_papers": {
    "answer": "- Hayashi et al. (2023) Existing comparative summarization works inspired the need for a more nuanced approach. (Methodological precursors)\n- Ströhle et al. (2023) Their two-step pipeline for comparative summarization highlighted the limitations of surface-level semantic differences. (Methodological precursors)",
    "evidence": "Existing comparative summarization works (Ströhle et al., 2023) typically follow a two-step pipeline: (1) construct extractive summaries for each document to (2) identify their similarities and differences (Lerman and McDonald, 2009; Gunel et al., 2024)."
  },
  "method": {
    "steps": [
      {
        "step": "Convert scientific papers into LLM personas for debate.",
        "input": "Scientific papers",
        "output": "LLM personas representing each paper",
        "evidence": "We introduce Tree-of-Debate (ToD), a framework which converts scientific papers into LLM personas that debate their respective novelties."
      },
      {
        "step": "Dynamically construct a debate tree.",
        "input": "LLM personas and their arguments",
        "output": "Debate tree with nodes representing specific contribution topics",
        "evidence": "ToD dynamically constructs a debate tree, enabling fine-grained analysis of independent novelty arguments within scholarly articles."
      },
      {
        "step": "Iterative retrieval of relevant segments during debates.",
        "input": "Debate content and retrieval queries",
        "output": "Targeted content for specific contributions",
        "evidence": "We propose an iterative retrieval process, where retrieval queries are dynamically determined by the debate’s content."
      }
    ],
    "tools": [
      {
        "name": "LLM agent",
        "description": "Used to embody each debate persona and integrate retrieved information into its context.",
        "evidence": "We leverage an LLM agent to embody each debate persona, allowing for retrieved information from the papers and the debate history to be easily integrated into its context."
      }
    ],
    "benchmark_datasets": [],
    "evaluation_metrics": [
      {
        "name": "Factuality",
        "purpose": "Measures how factual the summary is.",
        "application": "Each sentence is given a binary score for factuality, and the scores are averaged across the summary.",
        "evidence": "Factuality: How factual is the summary? Each sentence is given a 1/0 binary score for factuality, and the scores are averaged across the summary."
      },
      {
        "name": "Breadth",
        "purpose": "Measures the comprehensiveness and completeness of the summary.",
        "application": "Each summary is rated from 0-4 ('not at all' to 'very').",
        "evidence": "Breadth: Is the summary comprehensive and complete? Each summary is rated from 0-4 ('not at all' to 'very')."
      },
      {
        "name": "Contextualization",
        "purpose": "Measures how well the summary explains and justifies posed similarities/differences.",
        "application": "Each summary is rated from 0-4 ('not at all' to 'very').",
        "evidence": "Contextualization: Does the summary explain and/or justify the posed similarities/differences between the papers, as opposed to just mentioning them? Each summary is rated from 0-4 ('not at all' to 'very')."
      }
    ]
  },
  "method_type": {
    "methods": [
      {
        "name": "Hypothesis or Idea Generation",
        "description": "The system produces candidate hypotheses or new research ideas from prior knowledge or external input.",
        "evidence": "We propose converting scientific papers into personas that debate each other to foster critical analysis."
      },
      {
        "name": "Experimental design generation",
        "description": "The approach includes producing experimental protocols, configurations, or evaluation strategies.",
        "evidence": "We integrate these proposed principles into Tree-of-Debate, a framework which dynamically structures a debate between paper personas, conducted by a moderator."
      }
    ]
  },
  "subject_area": {
    "areas": [
      {
        "name": "Interdisciplinary Sciences",
        "description": "The paper develops a framework for comparative analysis across various scientific domains.",
        "evidence": "Through experiments on scientific literature across various domains, evaluated by expert researchers, we demonstrate that ToD generates informative arguments, effectively contrasts papers, and supports researchers in their literature review."
      }
    ]
  },
  "performance_summary": {
    "performance_summary": [
      {
        "summary": "Tree-of-Debate significantly improves contextualization, achieving an average score of 95.21% across all settings, compared to 75.57% for the strongest baseline.",
        "evidence": "Our results show that TREE-OF-DEBATE significantly improves contextualization, achieving an average score of 95.21% across all settings, compared to 75.57% for the strongest baseline."
      }
    ],
    "baselines": [
      {
        "name": "Single Stage",
        "description": "Uses the title, abstract, and introduction sections of both papers to directly generate a comparative summary.",
        "evidence": "Single Stage uses the title, abstract and introduction sections of both papers to directly generate a comparative summary."
      },
      {
        "name": "Two Stage",
        "description": "First individually summarizes each paper and then uses the generated summaries to create a comparative summary.",
        "evidence": "Two Stage first individually summarizes each paper based on the title, abstract and introductions, and then uses the generated summaries to generate a comparative summary."
      }
    ],
    "benchmark_datasets": [],
    "evaluation_metrics": [
      {
        "name": "Factuality",
        "purpose": "Measures how factual the summary is.",
        "application": "Each sentence is given a binary score for factuality, and the scores are averaged across the summary.",
        "evidence": "Factuality: How factual is the summary? Each sentence is given a 1/0 binary score for factuality, and the scores are averaged across the summary."
      },
      {
        "name": "Breadth",
        "purpose": "Measures the comprehensiveness and completeness of the summary.",
        "application": "Each summary is rated from 0-4 ('not at all' to 'very').",
        "evidence": "Breadth: Is the summary comprehensive and complete? Each summary is rated from 0-4 ('not at all' to 'very')."
      },
      {
        "name": "Contextualization",
        "purpose": "Measures how well the summary explains and justifies posed similarities/differences.",
        "application": "Each summary is rated from 0-4 ('not at all' to 'very').",
        "evidence": "Contextualization: Does the summary explain and/or justify the posed similarities/differences between the papers, as opposed to just mentioning them? Each summary is rated from 0-4 ('not at all' to 'very')."
      }
    ]
  },
  "benchmark_dataset": null,
  "limitations": {
    "limitations": [
      {
        "name": "Dependence on Model Pre-training",
        "description": "The quality of the debate and resulting summaries may vary based on the pre-training data of the LLMs used.",
        "evidence": "The quality of this critical response may vary based on the difficulty of the task (e.g., a more fine-grained topic that has no presence within the model’s existing pre-training dataset) and/or the size of the model."
      },
      {
        "name": "Potential for Hallucinations",
        "description": "Personas may suggest potential future studies or new methods, introducing noise to the summarization process.",
        "evidence": "While this does introduce some noise (as we can see through our competitive factuality scores, this is minimal) to our summarization process, these types of 'hallucinations' in the debate present exciting new paths for research to explore."
      }
    ]
  },
  "future_directions": {
    "future_directions": [
      {
        "name": "Extend to General Complex Reasoning Tasks",
        "description": "Apply the tree-based decomposition and debate-based critical feedback to other complex reasoning tasks.",
        "evidence": "Tree-of-Debate can be extended to other general, complex reasoning tasks which can exploit our tree-based decomposition and debate-based critical feedback."
      },
      {
        "name": "Explore Negotiation Settings",
        "description": "Consider extending the framework to negotiation settings where various aspects need to be considered for optimal compromise.",
        "evidence": "We can also consider extending this to a negotiation setting, where there are various aspects to consider when determining the optimal compromise."
      }
    ]
  },
  "resource_link": {
    "answer": "https://github.com/pkargupta/tree-of-debate",
    "evidence": "Reproducibility: We provide our dataset and source code1 to facilitate further studies. 1https://github.com/pkargupta/tree-of-debate"
  }
}