{
  "objective": {
    "answer": "The primary objective of the paper is to introduce SCI-IDEA, a framework that uses large language models (LLMs) for context-aware scientific ideation, focusing on generating high-quality, innovative ideas by leveraging LLM prompting strategies and 'Aha Moment' detection.",
    "evidence": "We introduce SCI-IDEA, a framework that uses LLM prompting strategies and 'Aha Moment' detection for iterative idea refinement."
  },
  "knowledge_gap": {
    "answer": "Existing methods for scientific ideation often fail to balance novelty, relevance, and computational efficiency, and lack systematic mechanisms to evaluate the novelty and impact of generated ideas.",
    "evidence": "Recently, there has been growing attention on leveraging LLMs for scientific ideation, yet existing methods often fail to balance novelty, relevance, and computational efficiency."
  },
  "novelty": {
    "answer": [
      "SCI-IDEA employs a dynamic evaluation process for novelty and surprise, enabling the identification of transformative ideas.",
      "The framework integrates iterative refinement and domain-specific embeddings to ensure the novelty, excitement, feasibility, and effectiveness of generated ideas."
    ],
    "evidence": [
      "SCI-IDEA addresses these limitations by dynamically evaluating novelty and surprise, enabling the identification of transformative ideas.",
      "SCI-IDEA addresses the limitations of existing methods by integrating iterative refinement, dynamic evaluation mechanisms, and domain-specific embeddings to ensure the novelty, excitement, feasibility, and effectiveness of generated ideas."
    ]
  },
  "inspirational_papers": {
    "answer": "- Si et al. (2024) Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. (Methodological precursors)",
    "evidence": "This multi-dimensional evaluation aligns with recent AI-assisted ideation studies [27], ensuring the assessment of strengths and limitations."
  },
  "method": {
    "steps": [
      {
        "step": "Context Retrieval",
        "input": "Researcher's scientific identifier and research query",
        "output": "Retrieval of the researcher's publications and related ones",
        "evidence": "This process begins with a researcher providing their scientific identifier (e.g., ORCID ID: 0000-0002-1825-0097) and a research query."
      },
      {
        "step": "Facet Extraction",
        "input": "Full texts from publications",
        "output": "Extraction of key facets such as research objectives, methodologies, evaluation, and future work",
        "evidence": "Following the retrieval of full texts from publications authored by the researcher using the framework and related publications, the framework leverages an LLM to extract key facets."
      },
      {
        "step": "Research Gap Identification",
        "input": "Structured facets from publications",
        "output": "Identification of research gaps",
        "evidence": "Using the structured facets extracted from the researchers and related research publications, the framework identifies research gaps by analyzing patterns, inconsistencies, and unexplored areas using the LLM."
      },
      {
        "step": "Idea Generation",
        "input": "Identified research gaps",
        "output": "Generation of context-aware scientific ideas",
        "evidence": "Building on identified research gaps, the framework generates context-aware scientific ideas using diverse prompting strategies."
      },
      {
        "step": "Evaluation of Novelty and Surprise",
        "input": "Generated ideas",
        "output": "Assessment of ideas for novelty and surprise",
        "evidence": "The generated ideas are evaluated for two metrics of novelty and surprise, using state-of-the-art semantic embedding approaches."
      },
      {
        "step": "Iterative Refinement",
        "input": "Ideas flagged as Aha moments",
        "output": "Refined ideas with enhanced novelty, feasibility, and impact",
        "evidence": "When an idea is flagged as an Aha moment, the Aha Prompt template is activated to guide deeper exploration."
      }
    ],
    "tools": [
      {
        "name": "Large Language Models (LLMs)",
        "description": "Used for generating context-aware scientific ideas and evaluating novelty and surprise",
        "evidence": "SCI-IDEA is a framework for generating, evaluating, and refining context-aware scientific ideas through an iterative, human-in-the-loop process."
      }
    ],
    "benchmark_datasets": [],
    "evaluation_metrics": [
      {
        "name": "Novelty",
        "purpose": "Measures the originality of generated ideas",
        "application": "Used to evaluate the distinctiveness of ideas compared to existing ones",
        "evidence": "The novelty of an idea ci is computed as: Novelty(ci) = 1 −max(Cosine_similarity(ci, cj)) ∀cj ∈C"
      },
      {
        "name": "Surprise",
        "purpose": "Measures how unexpected an idea is given the context",
        "application": "Used to assess the unexpectedness of ideas",
        "evidence": "Surprise of ci is quantified as: Surprise(ci) = −log p(ci | C)"
      }
    ]
  },
  "method_type": {
    "methods": [
      {
        "name": "Hypothesis or Idea Generation",
        "description": "The system produces candidate hypotheses or new research ideas from prior knowledge or external input.",
        "evidence": "Building on identified research gaps, the framework generates context-aware scientific ideas using diverse prompting strategies."
      },
      {
        "name": "Iterative refinement of Ideas, Hypothesis and Experiment design",
        "description": "The approach includes refining ideas to enhance novelty, feasibility, and impact.",
        "evidence": "When an idea is flagged as an Aha moment, the Aha Prompt template is activated to guide deeper exploration."
      }
    ]
  },
  "subject_area": {
    "areas": [
      {
        "name": "Interdisciplinary Sciences",
        "description": "The paper focuses on generating scientific ideas across various domains using LLMs.",
        "evidence": "SCI-IDEA has the potential to foster innovation and interdisciplinary collaboration across diverse scientific domains."
      }
    ]
  },
  "performance_summary": {
    "performance_summary": [
      {
        "summary": "SCI-IDEA achieves average scores of 6.84, 6.86, 6.89, and 6.84 across novelty, excitement, feasibility, and effectiveness, respectively.",
        "evidence": "Comprehensive experiments validate SCI-IDEA’s effectiveness, achieving average scores of 6.84, 6.86, 6.89, and 6.84 (on a 1–10 scale) across novelty, excitement, feasibility, and effectiveness, respectively."
      }
    ],
    "baselines": [
      {
        "name": "GPT-4o",
        "description": "Used as a baseline model for comparison in idea generation.",
        "evidence": "Evaluations employed GPT-4o, GPT-4.5, DeepSeek-32B (each under 2-shot prompting), and DeepSeek-70B (3-shot prompting)."
      },
      {
        "name": "DeepSeek-32B",
        "description": "Used as a baseline model for comparison in idea generation.",
        "evidence": "Evaluations employed GPT-4o, GPT-4.5, DeepSeek-32B (each under 2-shot prompting), and DeepSeek-70B (3-shot prompting)."
      }
    ],
    "benchmark_datasets": [],
    "evaluation_metrics": [
      {
        "name": "Novelty",
        "purpose": "Measures the originality of generated ideas",
        "application": "Used to evaluate the distinctiveness of ideas compared to existing ones",
        "evidence": "The novelty of an idea ci is computed as: Novelty(ci) = 1 −max(Cosine_similarity(ci, cj)) ∀cj ∈C"
      },
      {
        "name": "Surprise",
        "purpose": "Measures how unexpected an idea is given the context",
        "application": "Used to assess the unexpectedness of ideas",
        "evidence": "Surprise of ci is quantified as: Surprise(ci) = −log p(ci | C)"
      }
    ]
  },
  "benchmark_dataset": null,
  "limitations": {
    "limitations": [
      {
        "name": "Reliance on Input Data Quality",
        "description": "The framework's effectiveness is heavily dependent on the quality of input data, which may introduce biases.",
        "evidence": "The framework relies heavily on the quality of input data, such as the researcher’s and related publications, which may introduce biases or incomplete representations of the research landscape."
      },
      {
        "name": "Computational Cost",
        "description": "The computational cost of iterative refinement and embedding-based evaluations can be prohibitive for large-scale applications.",
        "evidence": "The computational cost of iterative refinement and embedding-based evaluations can be prohibitive for large-scale applications."
      }
    ]
  },
  "future_directions": {
    "future_directions": [
      {
        "name": "Incorporate More Diverse Datasets",
        "description": "Future work will focus on addressing limitations by incorporating more diverse datasets.",
        "evidence": "Future work will focus on addressing limitations by incorporating more diverse datasets, developing broader evaluation metrics, and optimizing computational efficiency."
      },
      {
        "name": "Integration of Domain-Specific Knowledge Graphs",
        "description": "Explore the integration of domain-specific knowledge graphs and multi-modal inputs to enhance the quality and diversity of generated ideas.",
        "evidence": "We also plan to explore the integration of domain-specific knowledge graphs and multi-modal inputs to enhance the quality and diversity of generated ideas."
      }
    ]
  },
  "resource_link": {
    "answer": "",
    "evidence": ""
  }
}