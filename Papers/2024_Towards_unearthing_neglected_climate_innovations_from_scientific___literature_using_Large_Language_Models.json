{
  "objective": {
    "answer": "The primary objective of the paper is to employ Large Language Models (LLMs) to identify neglected climate innovations within scientific literature, specifically focusing on UK-based solutions, and to assess the effectiveness of these models in comparison to human evaluations.",
    "evidence": "To address this gap, this study employs a curated dataset sourced from OpenAlex, a comprehensive repository of scientific papers. Utilising Large Language Models (LLMs), such as GPT4-o from OpenAI, we evaluate title-abstract pairs from scientific papers on seven dimensions, covering climate change mitigation potential, stage of technological development, and readiness for deployment."
  },
  "knowledge_gap": {
    "answer": "There is a gap in using LLMs to systematically identify climate innovations within a specific region, which this study addresses by focusing on the UK.",
    "evidence": "However, there is a gap in using LLMs to systematically identify climate innovations within a specific region. This study addresses this gap by using LLM evaluators to identify potential climate innovations in the UK, creating a workflow that can be applied to other regions and serve as the foundation for future work."
  },
  "novelty": {
    "answer": [
      "The use of LLMs to systematically identify climate innovations in scientific literature, specifically focusing on UK-based solutions.",
      "The comparison of LLM outputs with human evaluations to assess the effectiveness of LLMs in identifying overlooked innovations.",
      "The development of a workflow that is region-agnostic and can be applied to other regions."
    ],
    "evidence": [
      "This study addresses this gap by using LLM evaluators to identify potential climate innovations in the UK, creating a workflow that can be applied to other regions and serve as the foundation for future work.",
      "Benchmarking the outputs of the LLMs against parallel human evaluations, we aim to assess the effectiveness of these models in finding overlooked innovations and identify any potential advantages over human reasoning.",
      "Here, we focused on UK-based solutions, but the workflow is region-agnostic."
    ]
  },
  "inspirational_papers": {
    "answer": "- Priem et al. (2022) OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts. (Methodological precursors)",
    "evidence": "We leverage OpenAlex [8], a comprehensive open dataset of scholarly papers and comprehensive meta-data, to provide test data for analysis by state-of-the-art language models, such as GPT4-o from OpenAI."
  },
  "method": {
    "steps": [
      {
        "step": "Data collection from OpenAlex",
        "input": "OpenAlex database",
        "output": "A dataset of approximately 133,619 works",
        "evidence": "To collect relevant research papers, we utilised the OpenAlex database."
      },
      {
        "step": "Filtering and selection of abstracts",
        "input": "Dataset of 133,619 works",
        "output": "Final input dataset of 101,374 works",
        "evidence": "From this curated dataset, we removed works lacking a listed abstract text or topic classification metadata."
      },
      {
        "step": "Human evaluation via survey",
        "input": "100 selected abstracts",
        "output": "Binary responses to seven key questions",
        "evidence": "We implemented a survey using Qualtrics to collect human evaluations for the 100 selected abstracts."
      },
      {
        "step": "LLM evaluation",
        "input": "100 selected abstracts",
        "output": "Binary responses to seven key questions",
        "evidence": "In parallel with the human evaluation, we used a large language model (LLM) to analyse the same 100 abstracts and answer the same seven questions."
      }
    ],
    "tools": [
      {
        "name": "OpenAlex",
        "description": "Used for collecting relevant research papers",
        "evidence": "To collect relevant research papers, we utilised the OpenAlex database."
      },
      {
        "name": "GPT4-o",
        "description": "Used for evaluating title-abstract pairs from scientific papers",
        "evidence": "Utilising Large Language Models (LLMs), such as GPT4-o from OpenAI, we evaluate title-abstract pairs from scientific papers."
      }
    ],
    "benchmark_datasets": [],
    "evaluation_metrics": [
      {
        "name": "Cohen’s Kappa",
        "purpose": "Evaluates the level of agreement between two raters beyond what would be expected by chance",
        "application": "Computed for each of the seven questions to compare human responses with those generated by the three LLM scenarios",
        "evidence": "Cohen’s Kappa (κ), a statistical measure that evaluates the level of agreement between two raters beyond what would be expected by chance, was computed for each of the seven questions to compare human responses with those generated by the three LLM scenarios."
      }
    ]
  },
  "method_type": {
    "methods": [
      {
        "name": "Literature or Dataset Retrieval",
        "description": "The process involves retrieving relevant literature or datasets for analysis.",
        "evidence": "To collect relevant research papers, we utilised the OpenAlex database."
      },
      {
        "name": "Knowledge Extraction and Structurization",
        "description": "The approach includes extracting and structuring knowledge from the literature.",
        "evidence": "Utilising Large Language Models (LLMs), such as GPT4-o from OpenAI, we evaluate title-abstract pairs from scientific papers."
      }
    ]
  },
  "subject_area": {
    "areas": [
      {
        "name": "Earth & Environmental Sciences",
        "description": "The paper focuses on identifying climate innovations within scientific literature.",
        "evidence": "Climate change poses an urgent global threat, needing the rapid identification and deployment of innovative solutions."
      },
      {
        "name": "Interdisciplinary Sciences",
        "description": "The methodology involves the use of machine learning and LLMs, which are interdisciplinary tools.",
        "evidence": "This work contributes to the discovery of neglected innovations in scientific literature and demonstrates the potential of AI in enhancing climate action strategies."
      }
    ]
  },
  "performance_summary": {
    "performance_summary": [
      {
        "summary": "The LLM scenarios successfully identified the 5 positive control abstracts with known linkages to climate tech spin-outs, arguably more convincingly than the human survey participants.",
        "evidence": "The 5 positive control abstracts with known linkages to climate tech spin-outs were successfully identified by all three LLM scenarios (no shot, context, few-shot), arguably more convincingly than the 6 human survey participants."
      }
    ],
    "baselines": [],
    "benchmark_datasets": [],
    "evaluation_metrics": [
      {
        "name": "Cohen’s Kappa",
        "purpose": "Measures the level of agreement between two raters beyond what would be expected by chance",
        "application": "Computed for each of the seven questions to compare human responses with those generated by the three LLM scenarios",
        "evidence": "Cohen’s Kappa (κ), a statistical measure that evaluates the level of agreement between two raters beyond what would be expected by chance, was computed for each of the seven questions to compare human responses with those generated by the three LLM scenarios."
      }
    ]
  },
  "benchmark_dataset": null,
  "limitations": {
    "limitations": [
      {
        "name": "Limited Resolution of LLM Scoring",
        "description": "The deterministic reasoning and binary scoring of the LLM present a challenge for abstract identification, restricting the resolution of the algorithm.",
        "evidence": "Applying the calculated weightings to the human and LLM scoring of unknown abstracts, it is clear that the function of deterministic reasoning and a binary scoring presents a challenge for abstract identification."
      }
    ]
  },
  "future_directions": {
    "future_directions": [
      {
        "name": "Experiment with Different LLMs",
        "description": "Future work will involve experimenting with different LLMs beyond GPT-4o, such as LLaMA or other domain-specific models, to enhance the accuracy and diversity of responses.",
        "evidence": "Future work will involve experimenting with different LLMs beyond GPT-4o, such as LLaMA or other domain-specific models, to enhance the accuracy and diversity of responses."
      },
      {
        "name": "Incorporate Retrieval-Augmented Generation",
        "description": "Incorporate Retrieval-Augmented Generation (RAG) to provide the LLMs with a more extensive context, potentially increasing the quality of answers.",
        "evidence": "We will also incorporate Retrieval-Augmented Generation (RAG) to provide the LLMs with a more extensive context, potentially increasing the quality of answers."
      }
    ]
  },
  "resource_link": {
    "answer": "",
    "evidence": ""
  }
}