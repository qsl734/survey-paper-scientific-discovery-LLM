{
  "objective": {
    "answer": "The primary objective of the paper is to introduce Sample Design Engineering (SDE) as a methodical approach to enhance the post-tuning performance of Large Language Models (LLMs) by refining input, output, and reasoning designs.",
    "evidence": "This paper introduces Sample Design Engineering (SDE), a methodical approach to enhancing LLMs’ post-tuning performance by refining input, output, and reasoning designs."
  },
  "knowledge_gap": {
    "answer": "The paper addresses the largely unexplored challenge of designing effective training samples for fine-tuning LLMs, which is crucial for tailoring smaller open-source LLMs to specific requirements.",
    "evidence": "While numerous PE techniques have been developed for LLMs’ zero-shot and in-context learning (ICL), the challenge of designing effective training samples for fine-tuning LLMs—termed Sample Design Engineering (SDE) in this paper—remains underexplored."
  },
  "novelty": {
    "answer": [
      "Introduction of Sample Design Engineering (SDE) as a new approach to enhance LLMs' performance.",
      "Empirical evaluation of various sample design options and their impact on LLMs' downstream performance.",
      "Proposal of an integrated SDE strategy that combines the most effective design options.",
      "Exploratory analysis of the relationship between effective prompt and sample designs."
    ],
    "evidence": [
      "This paper introduces Sample Design Engineering (SDE), a methodical approach to enhancing LLMs’ post-tuning performance by refining input, output, and reasoning designs.",
      "We conduct a series of in-domain (ID) and out-of-domain (OOD) experiments to assess the impact of various design options on LLMs’ downstream performance.",
      "Leveraging these findings, we combine the empirically well-performing SDE options and propose an integrated SDE strategy ES-SDE.",
      "In an exploratory analysis, we investigate the link between effective prompt and sample designs, via perplexity, zero-shot, and ICL analysis."
    ]
  },
  "inspirational_papers": {
    "answer": "- Brown et al. (2020) Language models are few-shot learners. (Methodological precursors)\n- Wei et al. (2022) Chain-of-Thought prompting elicits reasoning in large language models. (Methodological precursors)",
    "evidence": "For instance, Brown et al. (2020) use carefully crafted prompts and in-context-learning (ICL) techniques to guide GPT-3 on novel tasks without training; Wei et al. (2022) propose the Chain-of-Thought (CoT) technique that can boost the logic reasoning performance."
  },
  "method": {
    "steps": [
      {
        "step": "Identify typical SDE options and categorize them into input, output, and reasoning design options.",
        "input": "Existing literature and design options for LLMs.",
        "output": "Categorized SDE options for experimentation.",
        "evidence": "We begin by identifying a range of typical SDE options and categorizing them into three groups: input, output, and reasoning design options."
      },
      {
        "step": "Conduct experiments on multi-aspect sentiment analysis (MASA) tasks to evaluate SDE options.",
        "input": "MASA tasks with 2 in-domain and 2 out-of-domain tasks.",
        "output": "Performance data on the impact of different SDE options.",
        "evidence": "To reveal the impact of each SDE option, we conduct experiments on a typical downstream scenario – multi-aspect sentiment analysis (MASA), with 2 in-domain (ID) tasks and 2 out-of-domain (OOD) tasks."
      },
      {
        "step": "Propose an integrated SDE strategy based on empirical findings.",
        "input": "Results from experiments on SDE options.",
        "output": "Integrated SDE strategy (ES-SDE).",
        "evidence": "Leveraging these findings, we combine the empirically well-performing SDE options and propose an integrated SDE strategy ES-SDE."
      }
    ],
    "tools": [
      {
        "name": "LoRA",
        "description": "Used as the default efficient fine-tuning technique.",
        "evidence": "We use LoRA as the default efficient fine-tuning technique."
      }
    ],
    "benchmark_datasets": [
      {
        "name": "GENIA",
        "data_description": "A nested named entity recognition dataset in the molecular biology domain.",
        "usage": "Used for evaluating the effectiveness of ES-SDE.",
        "evidence": "GENIA (Ohta et al., 2002). A nested named entity recognition (Nested-NER) dataset in the molecular biology domain."
      },
      {
        "name": "MAVEN",
        "data_description": "A general domain event detection dataset.",
        "usage": "Used for evaluating the effectiveness of ES-SDE.",
        "evidence": "MAVEN (Wang et al., 2020). A general domain event detection (ED) dataset."
      }
    ],
    "evaluation_metrics": [
      {
        "name": "Weighted Kappa",
        "purpose": "Measures classification performance considering the imbalance of different aspects and the ordinal nature of sentiment labels.",
        "application": "Used to evaluate MASA's performance.",
        "evidence": "We use the weighted Kappa score κ (Cohen, 1968) for this measurement considering the imbalance of different aspects and the ordinal nature of sentiment labels."
      },
      {
        "name": "Format adherence",
        "purpose": "Assesses the generation stability of LLMs.",
        "application": "Reported as the format-parsing error rate.",
        "evidence": "Format adherence, to assess the generation stability of LLMs."
      }
    ]
  },
  "method_type": {
    "methods": [
      {
        "name": "Hypothesis or Idea Generation",
        "description": "The system produces candidate hypotheses or new research ideas from prior knowledge or external input.",
        "evidence": "Our study is based on the hypothesis that the structure or elements of training samples may have a big impact on the fine-tuned LLMs."
      },
      {
        "name": "Experimental design generation",
        "description": "The approach includes producing experimental protocols, configurations, or evaluation strategies.",
        "evidence": "We conduct a series of in-domain (ID) and out-of-domain (OOD) experiments to assess the impact of various design options on LLMs’ downstream performance."
      }
    ]
  },
  "subject_area": {
    "areas": [
      {
        "name": "Interdisciplinary Sciences",
        "description": "The paper develops a method for enhancing LLMs' performance across various tasks, which involves elements of computer science and linguistics.",
        "evidence": "This paper introduces Sample Design Engineering (SDE), a methodical approach to enhancing LLMs’ post-tuning performance by refining input, output, and reasoning designs."
      }
    ]
  },
  "performance_summary": {
    "performance_summary": [
      {
        "summary": "The ES-SDE strategy consistently outperformed other methods across different tasks and training sizes.",
        "evidence": "ES-SDE maintains its advantage over other methods, across different tasks and training sizes."
      }
    ],
    "baselines": [
      {
        "name": "Heuristic sample design",
        "description": "A baseline method for sample design in LLM fine-tuning.",
        "evidence": "As a comparison to ES-SDE, we also propose an empirically weak SDE strategy (EW-SDE), combining Inst-last, Natural, and OU, while keeping other options the same."
      }
    ],
    "benchmark_datasets": [
      {
        "name": "GENIA",
        "data_description": "A nested named entity recognition dataset in the molecular biology domain.",
        "usage": "Used for evaluating the effectiveness of ES-SDE.",
        "evidence": "GENIA (Ohta et al., 2002). A nested named entity recognition (Nested-NER) dataset in the molecular biology domain."
      },
      {
        "name": "MAVEN",
        "data_description": "A general domain event detection dataset.",
        "usage": "Used for evaluating the effectiveness of ES-SDE.",
        "evidence": "MAVEN (Wang et al., 2020). A general domain event detection (ED) dataset."
      }
    ],
    "evaluation_metrics": [
      {
        "name": "Weighted Kappa",
        "purpose": "Measures classification performance considering the imbalance of different aspects and the ordinal nature of sentiment labels.",
        "application": "Used to evaluate MASA's performance.",
        "evidence": "We use the weighted Kappa score κ (Cohen, 1968) for this measurement considering the imbalance of different aspects and the ordinal nature of sentiment labels."
      },
      {
        "name": "Format adherence",
        "purpose": "Assesses the generation stability of LLMs.",
        "application": "Reported as the format-parsing error rate.",
        "evidence": "Format adherence, to assess the generation stability of LLMs."
      }
    ]
  },
  "benchmark_dataset": {
    "name": "GENIA",
    "data_description": "A nested named entity recognition dataset in the molecular biology domain.",
    "usage": "Used for evaluating the effectiveness of ES-SDE.",
    "evidence": "GENIA (Ohta et al., 2002). A nested named entity recognition (Nested-NER) dataset in the molecular biology domain."
  },
  "limitations": {
    "limitations": [
      {
        "name": "Limited Applicability to Untested Scenarios",
        "description": "The findings may not be applicable to more complex tasks with intricate structures.",
        "evidence": "While we demonstrate that the experimental findings from the first phase are extendable to different downstream tasks, the applicability to other untested scenarios remains uncertain."
      },
      {
        "name": "Rapid Advancements in LLMs",
        "description": "The models used in the study may be surpassed by newer releases, affecting the generalizability of the findings.",
        "evidence": "The models we used in our study were among the best open-source options available at the start of our research but have since been surpassed by newer releases."
      }
    ]
  },
  "future_directions": {
    "future_directions": [
      {
        "name": "Explore SDE in More Complex Tasks",
        "description": "Investigate the applicability of SDE strategies in more complex tasks with intricate structures.",
        "evidence": "Future research will address these more challenging contexts."
      },
      {
        "name": "Develop Efficient Framework for SDE Studies",
        "description": "Create a more efficient framework for conducting SDE studies to overcome current challenges.",
        "evidence": "Therefore, developing a more efficient framework for SDE studies is a critical objective for future research."
      }
    ]
  },
  "resource_link": {
    "answer": "https://github.com/beyondguo/LLM-Tuning",
    "evidence": "Code available at https://github.com/beyondguo/LLM-Tuning."
  }
}