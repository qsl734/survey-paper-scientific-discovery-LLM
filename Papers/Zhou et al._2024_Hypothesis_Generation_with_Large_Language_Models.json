{
  "objective": {
    "answer": "The primary objective of the paper is to explore the potential of large language models (LLMs) to generate high-quality hypotheses based on data, improving predictive performance in classification tasks.",
    "evidence": "In this paper, we examine the potential of large language models (LLMs) to generate hypotheses. We focus on hypothesis generation based on data (i.e., labeled examples)."
  },
  "knowledge_gap": {
    "answer": "The paper addresses the gap in automated hypothesis generation, which traditionally relies on researchers' manual efforts and lacks systematic computational approaches.",
    "evidence": "Despite the importance of hypothesis generation, as Ludwig and Mullainathan (2024) point out, science has been curiously asymmetric. While many scientific publications present extensive formal and empirical evaluation of hypotheses, the generation of hypotheses happens off-stage by researchers."
  },
  "novelty": {
    "answer": [
      "The paper introduces a novel computational framework for generating and evaluating hypotheses with LLMs.",
      "The proposed algorithm is inspired by the upper confidence bound algorithm in multi-armed bandits, introducing a reward function to guide hypothesis generation.",
      "The generated hypotheses enable interpretable hypothesis-based classifiers that outperform traditional few-shot and supervised learning methods."
    ],
    "evidence": [
      "We propose a novel computational framework for generating and evaluating hypotheses with LLMs.",
      "To generate high-quality hypotheses with LLMs, we propose an algorithm inspired by the upper confidence bound algorithm in multi-armed bandits.",
      "Our generated hypotheses enable interpretable hypothesis-based classifiers that outperform in-context learning and even supervised learning for one synthetic and three real-world datasets."
    ]
  },
  "inspirational_papers": {
    "answer": "- Auer (2002) The upper confidence bound algorithm inspired our reward function design. (Methodological precursors)\n- Ott et al. (2011) The DECEPTIVE REVIEWS dataset used in our experiments was inspired by their work on deception detection. (Experimental baselines)",
    "evidence": "Our hypothesis generation algorithm (Algorithm 1) is inspired by the upper confidence bound (UCB) algorithm (Auer, 2002). The dataset includes 800 genuine reviews and 800 fictitious reviews for 20 hotels in Chicago (Ott et al., 2011)."
  },
  "method": {
    "steps": [
      {
        "step": "Initialize hypothesis bank with initial examples.",
        "input": "Initial set of examples Sinit ⊂ S",
        "output": "Initial hypothesis bank H",
        "evidence": "Given a set of initial examples Sinit ⊂ S, we first prompt an LLM to generate hypotheses for Sinit, which serve as our initial hypothesis bank H."
      },
      {
        "step": "Evaluate and update hypotheses based on training examples.",
        "input": "Training examples and initial hypotheses",
        "output": "Updated hypothesis bank with rewards",
        "evidence": "In the update stage, for a training example s, we select the top k high-reward hypotheses from the hypothesis bank H."
      },
      {
        "step": "Generate new hypotheses from wrong example pool.",
        "input": "Wrong example pool W",
        "output": "New hypotheses to fill knowledge gaps",
        "evidence": "Once the wrong example pool reaches a max size of wmax, the wrong examples in W are used to generate new hypotheses."
      }
    ],
    "tools": [
      {
        "name": "Large Language Models (LLMs)",
        "description": "Used for generating initial and updated hypotheses.",
        "evidence": "We prompt an LLM to summarize demonstration examples into high-level hypotheses."
      }
    ],
    "benchmark_datasets": [
      {
        "name": "DECEPTIVE REVIEWS",
        "data_description": "800 genuine reviews and 800 fictitious reviews for 20 hotels in Chicago.",
        "usage": "Used for evaluating hypothesis generation and classification performance.",
        "evidence": "The dataset includes 800 genuine reviews and 800 fictitious reviews for 20 hotels in Chicago."
      }
    ],
    "evaluation_metrics": [
      {
        "name": "Accuracy",
        "purpose": "Measures the correctness of predictions.",
        "application": "Used to evaluate the performance of generated hypotheses on test datasets.",
        "evidence": "Since all our datasets are classification tasks with ground truth labels, we use accuracy as our evaluation metric."
      }
    ]
  },
  "method_type": {
    "methods": [
      {
        "name": "Hypothesis or Idea Generation",
        "description": "The system produces candidate hypotheses or new research ideas from prior knowledge or external input.",
        "evidence": "We prompt the LLM to generate testable hypotheses using domain-specific concepts derived from structured data."
      },
      {
        "name": "Experimental design generation",
        "description": "The approach includes producing experimental protocols, configurations, or evaluation strategies.",
        "evidence": "Our model proposes complete experimental setups including dataset split, evaluation metrics, and variables."
      }
    ]
  },
  "subject_area": {
    "areas": [
      {
        "name": "Social Sciences",
        "description": "The paper focuses on generating hypotheses for social science problems like deception detection and message popularity prediction.",
        "evidence": "The real-world tasks focus on deception detection and message popularity prediction, which are known to be challenging even for humans."
      },
      {
        "name": "Interdisciplinary Sciences",
        "description": "The methodology applies LLMs to hypothesis generation, bridging computational and social sciences.",
        "evidence": "We examine the potential of large language models (LLMs) to generate hypotheses."
      }
    ]
  },
  "performance_summary": {
    "performance_summary": [
      {
        "summary": "The proposed method improves accuracy by 31.7% on a synthetic dataset and by 13.9%, 3.3%, and 24.9% on three real-world datasets compared to few-shot prompting.",
        "evidence": "Our algorithm is able to generate hypotheses that enable much better predictive performance than few-shot prompting in classification tasks, improving accuracy by 31.7% on a synthetic dataset and by 13.9%, 3.3% and, 24.9% on three real-world datasets."
      }
    ],
    "baselines": [
      {
        "name": "Few-shot prompting",
        "description": "A baseline method using LLMs with task-specific instructions and examples.",
        "evidence": "We provide LLMs with task-specific instructions (zero-shot), optionally accompanied by three demonstration examples (few-shot)."
      },
      {
        "name": "Supervised Learning",
        "description": "Fine-tuning models like RoBERTa and Llama-2-7B on datasets.",
        "evidence": "We fine-tune RoBERTa (Liu et al., 2019) and Llama-2-7B (Touvron et al., 2023) on each of the datasets to serve as a non-interpretable oracle."
      }
    ],
    "benchmark_datasets": [
      {
        "name": "DECEPTIVE REVIEWS",
        "data_description": "800 genuine reviews and 800 fictitious reviews for 20 hotels in Chicago.",
        "usage": "Used for evaluating hypothesis generation and classification performance.",
        "evidence": "The dataset includes 800 genuine reviews and 800 fictitious reviews for 20 hotels in Chicago."
      }
    ],
    "evaluation_metrics": [
      {
        "name": "Accuracy",
        "purpose": "Measures correct classifications over total predictions.",
        "application": "Used to compare all models on held-out test data.",
        "evidence": "Since all our datasets are classification tasks with ground truth labels, we use accuracy as our evaluation metric."
      }
    ]
  },
  "benchmark_dataset": {
    "name": "DECEPTIVE REVIEWS",
    "data_description": "800 genuine reviews and 800 fictitious reviews for 20 hotels in Chicago.",
    "usage": "Used for evaluating hypothesis generation and classification performance.",
    "evidence": "The dataset includes 800 genuine reviews and 800 fictitious reviews for 20 hotels in Chicago."
  },
  "limitations": {
    "limitations": [
      {
        "name": "Limited Generalizability",
        "description": "The generated hypotheses may not generalize well to tasks outside the tested domains.",
        "evidence": "Our main goal is to generate high-quality hypotheses rather than maximizing the performance of this particular way of using the hypotheses."
      },
      {
        "name": "Dependence on LLMs",
        "description": "The approach relies heavily on the capabilities of LLMs, which may not always produce accurate or relevant hypotheses.",
        "evidence": "Although the utility of hypotheses in assisting downstream classification serves as an indicator for LLMs’ ability to generate hypotheses, our goal is not to maximize the classification performance."
      }
    ]
  },
  "future_directions": {
    "future_directions": [
      {
        "name": "Extend to Natural Sciences",
        "description": "Explore hypothesis generation in natural science domains, which may require symbolic parsers.",
        "evidence": "We leave extending our framework to natural science tasks as future work."
      },
      {
        "name": "Improve Hypothesis Quality",
        "description": "Conduct a more thorough hyperparameter search to enhance the quality of generated hypotheses.",
        "evidence": "We believe that a more thorough hyperparameter search could improve the performance of our methodology."
      }
    ]
  },
  "resource_link": {
    "answer": "https://github.com/ChicagoHAI/hypothesis-generation",
    "evidence": "We have publicly released the code and data for HypoGeniC at https://github.com/ChicagoHAI/hypothesis-generation."
  }
}