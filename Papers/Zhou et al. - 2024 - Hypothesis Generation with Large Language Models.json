{
  "objective": {
    "answer": "The primary objective of the paper is to examine the potential of large language models to generate high-quality, interpretable hypotheses based on labeled data. The authors aim to develop a computational framework that enables large language models to iteratively generate and refine hypotheses, improving their quality and utility for downstream classification tasks. They focus on creating a method that not only enhances predictive performance but also produces hypotheses that are interpretable and can generalize across models and datasets.",
    "evidence": "In this paper, we examine the potential of large language models (LLMs) to generate hypotheses. We focus on hypothesis generation based on data (i.e., labeled examples). To enable LLMs to handle long contexts, we generate initial hypotheses from a small number of examples and then update them iteratively to improve the quality of hypotheses."
  },
  "knowledge_gap": {
    "answer": "While hypothesis generation is central to scientific progress, it has traditionally been an off-stage, manual process by researchers, with little formalization or automation, especially in leveraging large language models for data-driven hypothesis generation and evaluation.",
    "evidence": "While many scientific publications present extensive formal and empirical evaluation of hypotheses, the generation of hypotheses happens off-stage by researchers. ... While one can easily prompt LLMs to generate hypotheses, LLMs may not be able to effectively leverage the input examples in a single long prompt. Moreover, it is important to have measures of quality in the generation process so that we can filter bad hypotheses and come up with better ones."
  },
  "novelty": {
    "answer": [
      "A novel computational framework (HypoGeniC) for automated, data-driven hypothesis generation and evaluation using large language models.",
      "An iterative update mechanism inspired by the upper confidence bound algorithm from multi-armed bandits to balance exploration and exploitation in hypothesis refinement.",
      "A reward function that guides hypothesis selection and generation based on predictive accuracy and exploration.",
      "Development of interpretable, hypothesis-based classifiers that outperform few-shot and even supervised learning baselines on challenging real-world tasks.",
      "Demonstration that hypotheses generated by one large language model can be used for inference by another, showing cross-model generalizability."
    ],
    "evidence": [
      "We propose a novel computational framework for generating and evaluating hypotheses with LLMs.",
      "To generate high-quality hypotheses with LLMs, we propose an algorithm inspired by the upper confidence bound algorithm in multi-armed bandits (Auer, 2002) (HypoGeniC1, Hypothesis Generation in Context; see Figure 1).",
      "We introduce a reward function and evaluate the top k hypotheses for each training example.",
      "Our generated hypotheses enable interpretable hypothesis-based classifiers that outperform in-context learning and even supervised learning for one synthetic and three real-world datasets.",
      "We show that hypotheses generated by one LLM (e.g., GPT-3.5-turbo) can be used to make accurate inference by another LLM (e.g., Mixtral)."
    ]
  },
  "inspirational_papers": {
    "answer": [
      "Auer (2002) The upper confidence bound algorithm inspired the exploration-exploitation tradeoff in hypothesis refinement. (Methodological precursors)",
      "Kaplan et al. (2020) The scaling law hypothesis in language modeling inspired the context of hypothesis generation. (Methodological precursors)",
      "Ott et al. (2011) Provided the DECEPTIVE REVIEWS dataset and baseline for deception detection. (Experimental baselines)",
      "Salganik et al. (2006) Provided background and challenge context for popularity prediction tasks. (Experimental baselines)",
      "Qiu et al. (2024) Motivated testing the ability of large language models to perform human-like induction reasoning. (Methodological precursors)",
      "Zhong et al. (2023) Aimed to support open-ended exploration via language descriptions. (Methodological precursors)"
    ],
    "evidence": [
      "To generate high-quality hypotheses with LLMs, we propose an algorithm inspired by the upper confidence bound algorithm in multi-armed bandits (Auer, 2002)",
      "In the context of language modeling, the hypothesis on scaling law inspires recent progress in large language models (LLMs) (Kaplan et al., 2020).",
      "The real-world tasks focus on deception detection and message popularity prediction, which are known to be challenging even for humans (Ott et al., 2011; Salganik et al., 2006).",
      "Our work is connected to many recent studies on using LLMs to propose “hypotheses”, notably, Qiu et al. (2024) and Zhong et al. (2023). Qiu et al. (2024) is motivated by testing the ability of LLMs to perform human-like induction reasoning, and Zhong et al. (2023) aims to support open-ended exploration."
    ]
  },
  "method": {
    "steps": [
      {
        "step": "Initialization of Hypothesis Bank",
        "input": "A small subset of labeled examples (Sinit ⊂ S), number of initial hypotheses to generate (num_init)",
        "output": "Initial hypothesis bank H containing hypotheses generated by prompting a large language model",
        "tools": [
          "Large language model (e.g., GPT-3.5-turbo, Mixtral, Claude-2.1): Used to generate initial hypotheses from data examples."
        ],
        "evidence": "Given a set of initial examples Sinit ⊂S, we first prompt an LLM to generate hypotheses for Sinit, which serve as our initial hypothesis bank H."
      },
      {
        "step": "Iterative Hypothesis Update",
        "input": "Training examples (S), current hypothesis bank H, reward function parameters, wrong example bank W, top k hypotheses (k), maximum wrong example bank size (wmax)",
        "output": "Updated hypothesis bank H with refined and new hypotheses, updated rewards for each hypothesis",
        "tools": [
          "Large language model: Used to make predictions with each hypothesis and to generate new hypotheses from wrong examples."
        ],
        "evidence": "In the update stage, for a training example s, we select the top k high-reward hypotheses from the hypothesis bank H. The LLM is prompted to make a prediction with each of the top k high-reward hypotheses on s. ... If whyp hypotheses predict incorrectly for the example s, then s is added to a wrong example pool W. Once the wrong example pool reaches a max size of wmax, the wrong examples in W are used to generate new hypotheses."
      },
      {
        "step": "Reward Calculation and Hypothesis Selection",
        "input": "Prediction correctness for each hypothesis on each example, number of times each hypothesis has been selected, training time step (t), exploration coefficient (α)",
        "output": "Updated reward values for each hypothesis, selection of top k hypotheses for next iteration",
        "tools": [
          "Reward function (UCB-inspired): Balances exploitation (accuracy) and exploration (selection frequency) for each hypothesis."
        ],
        "evidence": "Formally, the reward is defined as ri = P(xj,yj)∈Si I(yj = ˆyj)/|Si| + α sqrt(log t/|Si|), where Si is the set of examples that have been used to evaluate the hypothesis hi, t is train time step, and α is a hyperparameter that controls the exploration term."
      },
      {
        "step": "Hypothesis-based Inference",
        "input": "Final hypothesis bank H, test examples, selected inference strategy (best-accuracy, filter and weighted vote, single-step adaptive, two-step adaptive)",
        "output": "Predicted labels for test examples based on hypotheses",
        "tools": [
          "Large language model: Used to apply hypotheses to new examples and make predictions.",
          "Inference strategies: Various prompt-based approaches to aggregate or select hypotheses for prediction."
        ],
        "evidence": "We propose a suite of inference strategies given a set of hypotheses. ... We develop multiple inference strategies to account for these different styles of reasoning (see Appendix A for prompts and Appendix B.2 for implementation details)."
      }
    ],
    "tools": [
      "Large language models (GPT-3.5-turbo, Mixtral, Claude-2.1): Used for hypothesis generation, prediction, and inference.",
      "Reward function (UCB-inspired): Guides hypothesis selection and exploration.",
      "Prompt engineering: Custom prompts for hypothesis generation and inference.",
      "Supervised learning models (RoBERTa, Llama-2-7B): Used as oracles/baselines for comparison."
    ],
    "evidence": [
      "Given a set of initial examples Sinit ⊂S, we first prompt an LLM to generate hypotheses for Sinit, which serve as our initial hypothesis bank H.",
      "In the update stage, for a training example s, we select the top k high-reward hypotheses from the hypothesis bank H. The LLM is prompted to make a prediction with each of the top k high-reward hypotheses on s.",
      "Formally, the reward is defined as ri = P(xj,yj)∈Si I(yj = ˆyj)/|Si| + α sqrt(log t/|Si|), where Si is the set of examples that have been used to evaluate the hypothesis hi, t is train time step, and α is a hyperparameter that controls the exploration term.",
      "We propose a suite of inference strategies given a set of hypotheses. We apply our method to one synthetic task where there is a single known valid hypothesis and three real-world tasks (DECEPTIVE REVIEWS, HEADLINE POPULARITY, and TWEET POPULARITY)."
    ]
  },
  "subject_area": {
    "areas": [
      "Social Sciences"
    ],
    "evidence": [
      "The real-world tasks focus on deception detection and message popularity prediction, which are known to be challenging even for humans (Ott et al., 2011; Salganik et al., 2006).",
      "Our experiments demonstrate the potential of LLMs in generating hypotheses for social science research to discover unknown knowledge in the data."
    ]
  },
  "performance_summary": {
    "performance_summary": [
      "HypoGeniC outperforms zero-shot and few-shot prompting by 31.7% on SHOE SALES, 13.9% on DECEPTIVE REVIEWS, 3.3% on HEADLINE POPULARITY, and 24.9% on TWEET POPULARITY.",
      "HypoGeniC matches or exceeds fine-tuned supervised models (RoBERTa, Llama-2-7B) on most datasets, with improvements of 12.8% and 11.2% over RoBERTa, and 12.1% and 11.6% over Llama-2-7B on HEADLINE POPULARITY and TWEET POPULARITY, respectively.",
      "On an out-of-distribution dataset for DECEPTIVE REVIEWS, HypoGeniC outperforms the oracle fine-tuned RoBERTa by 1.7%.",
      "Updating the hypothesis bank leads to higher quality hypotheses, with improvements over the 'no updates' baseline by 0.7% to 8.1% depending on the dataset."
    ],
    "baselines": [
      "Zero-shot prompting: LLMs are provided with task-specific instructions only.",
      "Few-shot prompting: LLMs are provided with task-specific instructions and three demonstration examples.",
      "No updates: Uses only the initial hypotheses generated, without iterative updates.",
      "Supervised learning: Fine-tuned RoBERTa and Llama-2-7B models trained on the datasets."
    ],
    "benchmark_datasets": [
      "DECEPTIVE REVIEWS: Contains 800 genuine and 800 fictitious hotel reviews for 20 hotels in Chicago; used for deception detection.",
      "HEADLINE POPULARITY: Derived from the Upworthy Research Archive, consists of 150,816 headlines across 22,666 packages; used for predicting which headline in a pair received more clicks.",
      "TWEET POPULARITY: Contains 13,174 tweet pairs matched by topic and author; used for predicting which tweet received more retweets."
    ],
    "evaluation_metrics": [
      "Accuracy: The proportion of correct predictions out of total predictions, used to evaluate classification performance on held-out test sets."
    ],
    "evidence": [
      "Across all LLMs, HypoGeniC outperforms the zero-shot learning by an average of 60% on SHOE SALES, 22.7% on DECEPTIVE REVIEWS, 5.1% on HEADLINE POPULARITY, and 30.6% on TWEET POPULARITY. Similarly, we find that HypoGeniC shows an increase from few-shot learning by 31.7% on SHOE SALES, 13.9% on DECEPTIVE REVIEWS, 3.3% on HEADLINE POPULARITY, and 24.9% on TWEET POPULARITY.",
      "HypoGeniC matches or even exceeds the fine-tuned models with the same number of training examples on most datasets. ... HypoGeniC is 12.8% and 11.2% better than RoBERTa, and 12.1% and 11.6% better than Llama-2-7B, on HEADLINE POPULARITY and TWEET POPULARITY respectively with 200 training examples.",
      "We use three different LLMs in our experiments (Mixtral (Mistral, 2023), GPT-3.5-turbo (OpenAI, 2023a), and Claude-2.1 (Anthropic, 2023)). We compare our approach with the following methods. 1. Zero-shot and few-shot prompting. ... 2. No updates. ... 3. Supervised Learning. We fine-tune RoBERTa (Liu et al., 2019) and Llama-2-7B (Touvron et al., 2023) on each of the datasets to serve as a non-interpretable oracle.",
      "The dataset includes 800 genuine reviews and 800 fictitious reviews for 20 hotels in Chicago.",
      "We use two datasets in this work: HEADLINE POPULARITY and TWEET POPULARITY. HEADLINE POPULARITY is derived from a dataset in the Upworthy Research Archive (Matias et al., 2021). ... TWEET POPULARITY uses a dataset of 13,174 tweet pairs (Tan et al., 2014), which are matched by the topic and the author.",
      "Since all our datasets are classification tasks with ground truth labels, we use accuracy as our evaluation metric."
    ]
  },
  "limitations": {
    "limitations": [
      {
        "label": "Domain Restriction",
        "explanation": "The experiments are limited to social science tasks, and the method has not been tested on natural science domains.",
        "evidence": "Q: Why only experiment with social science tasks? A: Math and physics problems and hypotheses are hard to represent in natural language and usually require symbolic parsers (Trinh et al., 2024). ... We leave extending our framework to natural science tasks as future work."
      },
      {
        "label": "Limited Hyperparameter Search",
        "explanation": "The framework did not undergo extensive hyperparameter optimization, which may affect performance.",
        "evidence": "Q: What hyperparameters have you tried? A: We aim to provide a robust framework for hypothesis generation, as opposed to focusing on the optimization of results. Thus, we did not perform an extensive hyperparameter search with the generation portion of HypoGeniC."
      },
      {
        "label": "High Latency and Cost",
        "explanation": "The approach can be time-consuming and financially costly, especially with multiple prompt-based inference strategies.",
        "evidence": "Q: How costly is your approach? A: HypoGeniC has high latency, specifically when using inference methods that require multiple prompts. ... the procedure can be time-consuming and require financial costs (e.g., GPT-3.5-turbo takes $2.05 on average over 76 experiments with an average of 1.5 hours per experiment)."
      },
      {
        "label": "Potential for Bias and Ethical Risks",
        "explanation": "There is a risk that generated hypotheses may reinforce stereotypes or reveal personal information if not properly overseen.",
        "evidence": "Q: What are some potential risks of hypothesis generation? A: One potential risk of hypothesis generation is that there is little guard regarding steorotypes and biases being confirmed if given data that may seem to enforce them. ... We highly recommend human-AI collaboration in using HypoGeniC to ensure that the generated hypotheses are ethical and unbiased."
      }
    ],
    "evidence": [
      "Q: Why only experiment with social science tasks? A: Math and physics problems and hypotheses are hard to represent in natural language and usually require symbolic parsers (Trinh et al., 2024). ... We leave extending our framework to natural science tasks as future work.",
      "Q: What hyperparameters have you tried? A: We aim to provide a robust framework for hypothesis generation, as opposed to focusing on the optimization of results. Thus, we did not perform an extensive hyperparameter search with the generation portion of HypoGeniC.",
      "Q: How costly is your approach? A: HypoGeniC has high latency, specifically when using inference methods that require multiple prompts. ... the procedure can be time-consuming and require financial costs (e.g., GPT-3.5-turbo takes $2.05 on average over 76 experiments with an average of 1.5 hours per experiment).",
      "Q: What are some potential risks of hypothesis generation? A: One potential risk of hypothesis generation is that there is little guard regarding steorotypes and biases being confirmed if given data that may seem to enforce them. ... We highly recommend human-AI collaboration in using HypoGeniC to ensure that the generated hypotheses are ethical and unbiased."
    ]
  },
  "future_directions": {
    "future_directions": [
      "Extend the hypothesis generation framework to natural science tasks, which may require symbolic representations and parsers.",
      "Conduct a more thorough hyperparameter search to potentially improve the performance of the methodology.",
      "Explore hypothesis generation that requires additional modalities and/or leverages existing literature along with past observations.",
      "Investigate the ideal hypothesis bank size and the effect of other hyperparameters on performance."
    ],
    "evidence": [
      "We leave extending our framework to natural science tasks as future work.",
      "We believe that a more thorough hyperparameter search could improve the performance of our methodology.",
      "We encourage future work to explore hypothesis generation that requires additional modalities and/or leverages existing literature along with past observations.",
      "The ideal hypothesis bank size may require further investigation."
    ]
  },
  "resource_link": {
    "answer": "https://github.com/ChicagoHAI/hypothesis-generation",
    "evidence": "We have publicly released the code and data for HypoGeniC at https://github.com/ChicagoHAI/hypothesis-generation."
  },
  "paper_title": "Hypothesis Generation with Large Language Models",
  "authors": [
    "Yangqiaoyu",
    "Haokun",
    "Tejes",
    "Hongyuan",
    "Chenhao"
  ],
  "published": "2024",
  "link": "http://arxiv.org/abs/2404.04326"
}