{
  "objective": {
    "answer": "The primary objective of the paper is to explore the potential of large language models (LLMs) to generate high-quality hypotheses based on data, improving predictive performance in classification tasks.",
    "evidence": "In this paper, we examine the potential of large language models (LLMs) to generate hypotheses. We focus on hypothesis generation based on data (i.e., labeled examples)."
  },
  "knowledge_gap": {
    "answer": "The paper addresses the gap in automated hypothesis generation, which traditionally relies on researchers' manual efforts and lacks systematic computational approaches.",
    "evidence": "Despite the importance of hypothesis generation, as Ludwig and Mullainathan (2024) point out, science has been curiously asymmetric. While many scientific publications present extensive formal and empirical evaluation of hypotheses, the generation of hypotheses happens off-stage by researchers."
  },
  "novelty": {
    "answer": [
      "The paper introduces a novel computational framework for generating and evaluating hypotheses with LLMs.",
      "The proposed algorithm is inspired by the upper confidence bound algorithm in multi-armed bandits to balance exploration and exploitation in hypothesis generation.",
      "The method includes a wrong example bank to capture knowledge gaps and iteratively improve hypothesis quality."
    ],
    "evidence": [
      "We propose a novel computational framework for generating and evaluating hypotheses with LLMs.",
      "Our hypothesis generation algorithm (Algorithm 1) is inspired by the upper confidence bound (UCB) algorithm (Auer, 2002).",
      "We maintain a wrong example bank to capture the gap in knowledge of the hypotheses pool, and generate new hypotheses based on the wrong example bank to close the gap."
    ]
  },
  "inspirational_papers": {
    "answer": "- Auer (2002) The upper confidence bound algorithm inspired our reward function for hypothesis generation. (Methodological precursors)",
    "evidence": "Our hypothesis generation algorithm (Algorithm 1) is inspired by the upper confidence bound (UCB) algorithm (Auer, 2002)."
  },
  "method": {
    "steps": [
      {
        "step": "Initialize hypothesis bank with initial examples.",
        "input": "Initial set of examples Sinit",
        "output": "Initial hypothesis bank H",
        "evidence": "Given a set of initial examples Sinit âŠ‚S, we first prompt an LLM to generate hypotheses for Sinit, which serve as our initial hypothesis bank H."
      },
      {
        "step": "Evaluate and update hypotheses using training examples.",
        "input": "Training examples and initial hypotheses",
        "output": "Updated hypothesis bank with rewards",
        "evidence": "In the update stage, for a training example s, we select the top k high-reward hypotheses from the hypothesis bank H."
      },
      {
        "step": "Generate new hypotheses from wrong example bank.",
        "input": "Wrong example bank W",
        "output": "New hypotheses to fill knowledge gaps",
        "evidence": "Once the wrong example pool reaches a max size of wmax, the wrong examples in W are used to generate new hypotheses."
      }
    ],
    "tools": [
      {
        "name": "Large Language Models (LLMs)",
        "description": "Used for generating initial and updated hypotheses.",
        "evidence": "We prompt an LLM to summarize demonstration examples into high-level hypotheses."
      }
    ],
    "benchmark_datasets": [
      {
        "name": "DECEPTIVE REVIEWS",
        "data_description": "800 genuine reviews and 800 fictitious reviews for 20 hotels in Chicago.",
        "usage": "Used for evaluating hypothesis generation in deception detection.",
        "evidence": "The dataset includes 800 genuine reviews and 800 fictitious reviews for 20 hotels in Chicago."
      },
      {
        "name": "HEADLINE POPULARITY",
        "data_description": "150,816 headlines across 22,666 packages from the Upworthy Research Archive.",
        "usage": "Used for evaluating hypothesis generation in predicting headline popularity.",
        "evidence": "HEADLINE POPULARITY is derived from a dataset in the Upworthy Research Archive."
      },
      {
        "name": "TWEET POPULARITY",
        "data_description": "13,174 tweet pairs matched by topic and author.",
        "usage": "Used for evaluating hypothesis generation in predicting tweet popularity.",
        "evidence": "TWEET POPULARITY uses a dataset of 13,174 tweet pairs (Tan et al., 2014), which are matched by the topic and the author."
      }
    ],
    "evaluation_metrics": [
      {
        "name": "Accuracy",
        "purpose": "Measures the correctness of predictions.",
        "application": "Used to evaluate the performance of generated hypotheses on test datasets.",
        "evidence": "Since all our datasets are classification tasks with ground truth labels, we use accuracy as our evaluation metric."
      }
    ]
  },
  "method_type": {
    "methods": [
      {
        "name": "Hypothesis or Idea Generation",
        "description": "The system produces candidate hypotheses or new research ideas from prior knowledge or external input.",
        "evidence": "We prompt the LLM to generate testable hypotheses using domain-specific concepts derived from structured data."
      },
      {
        "name": "Experimental design generation",
        "description": "The approach includes producing experimental protocols, configurations, or evaluation strategies.",
        "evidence": "Our model proposes complete experimental setups including dataset split, evaluation metrics, and variables."
      }
    ]
  },
  "subject_area": {
    "areas": [
      {
        "name": "Social Sciences",
        "description": "The paper focuses on hypothesis generation for social science problems like deception detection and message popularity prediction.",
        "evidence": "The real-world tasks focus on deception detection and message popularity prediction, which are known to be challenging even for humans."
      },
      {
        "name": "Interdisciplinary Sciences",
        "description": "The methodology applies LLMs to generate hypotheses across various domains, bridging AI and empirical sciences.",
        "evidence": "We examine the potential of large language models (LLMs) to generate hypotheses."
      }
    ]
  },
  "performance_summary": {
    "performance_summary": [
      {
        "summary": "The proposed method outperformed few-shot prompting by 31.7% on a synthetic dataset and by 13.9%, 3.3%, and 24.9% on three real-world datasets.",
        "evidence": "Our algorithm is able to generate hypotheses that enable much better predictive performance than few-shot prompting in classification tasks, improving accuracy by 31.7% on a synthetic dataset and by 13.9%, 3.3% and, 24.9% on three real-world datasets."
      }
    ],
    "baselines": [
      {
        "name": "Few-shot prompting",
        "description": "Baseline method using LLMs with few examples.",
        "evidence": "We compare its performance with standard supervised learning with the fine-tuned models and few-shot in-context learning."
      },
      {
        "name": "Supervised Learning",
        "description": "Fine-tuned models like RoBERTa and Llama-2-7B.",
        "evidence": "We fine-tune RoBERTa (Liu et al., 2019) and Llama-2-7B (Touvron et al., 2023) on each of the datasets to serve as a non-interpretable oracle."
      }
    ],
    "benchmark_datasets": [
      {
        "name": "DECEPTIVE REVIEWS",
        "data_description": "800 genuine reviews and 800 fictitious reviews for 20 hotels in Chicago.",
        "usage": "Used for evaluating hypothesis generation in deception detection.",
        "evidence": "The dataset includes 800 genuine reviews and 800 fictitious reviews for 20 hotels in Chicago."
      },
      {
        "name": "HEADLINE POPULARITY",
        "data_description": "150,816 headlines across 22,666 packages from the Upworthy Research Archive.",
        "usage": "Used for evaluating hypothesis generation in predicting headline popularity.",
        "evidence": "HEADLINE POPULARITY is derived from a dataset in the Upworthy Research Archive."
      },
      {
        "name": "TWEET POPULARITY",
        "data_description": "13,174 tweet pairs matched by topic and author.",
        "usage": "Used for evaluating hypothesis generation in predicting tweet popularity.",
        "evidence": "TWEET POPULARITY uses a dataset of 13,174 tweet pairs (Tan et al., 2014), which are matched by the topic and the author."
      }
    ],
    "evaluation_metrics": [
      {
        "name": "Accuracy",
        "purpose": "Measures the correctness of predictions.",
        "application": "Used to evaluate the performance of generated hypotheses on test datasets.",
        "evidence": "Since all our datasets are classification tasks with ground truth labels, we use accuracy as our evaluation metric."
      }
    ]
  },
  "benchmark_dataset": {
    "name": null,
    "description": "No traditional benchmark dataset was used.",
    "usage": "The study used author-collected datasets specific to the tasks.",
    "evidence": "The datasets used are specific to the tasks and not traditional benchmarks."
  },
  "limitations": {
    "limitations": [
      {
        "name": "Limited Generalizability",
        "description": "The generated hypotheses may not generalize well to tasks outside the tested domains.",
        "evidence": "We leave extending our framework to natural science tasks as future work."
      },
      {
        "name": "Potential Bias in Hypotheses",
        "description": "The generated hypotheses might reflect biases present in the training data.",
        "evidence": "One potential risk of hypothesis generation is that there is little guard regarding stereotypes and biases being confirmed if given data that may seem to enforce them."
      }
    ]
  },
  "future_directions": {
    "future_directions": [
      {
        "name": "Extend to Natural Sciences",
        "description": "Explore hypothesis generation for tasks in natural sciences.",
        "evidence": "We leave extending our framework to natural science tasks as future work."
      },
      {
        "name": "Improve Hypothesis Quality",
        "description": "Conduct a more thorough hyperparameter search to enhance hypothesis generation.",
        "evidence": "We believe that a more thorough hyperparameter search could improve the performance of our methodology."
      }
    ]
  },
  "resource_link": {
    "answer": "https://github.com/ChicagoHAI/hypothesis-generation",
    "evidence": "We have publicly released the code and data for HypoGeniC at https://github.com/ChicagoHAI/hypothesis-generation."
  }
}